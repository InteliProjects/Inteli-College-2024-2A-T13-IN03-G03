{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importação das Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, estimate_bandwidth\n",
        "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# from hdbscan import HDBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9OyZXxejzR"
      },
      "source": [
        "1. Neste bloco realizamos a importação das bibliotecas utilizadas para análise dos dados e treinamento do modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carregar a Base de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I9xHXKqml-jL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../documents/extras/base_dados.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Nesta etapa carregamos a base de dados da Unipar para efetuarmos todas as análises.\n",
        "    - Segue o _link_ da base de dados para facilitar a correção **(Só é possível ter acesso a este arquivo caso você esteja autenticado com uma conta com o e-mail institucional do Inteli)**: [Base de Dados](https://drive.google.com/file/d/18xMCvMC2UTvYebN-ZXvJw_0eZM8WCu4y/view?usp=sharing)\n",
        "    - Posicione o arquivo base_dados.csv na pasta **/documents/extras/**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Identificação das colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pwNbx48DJCv"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D7n-SAprRfv"
      },
      "source": [
        "1. Nesta Etapa tipificamos as colunas como numéricas ou categóricas para prosseguirmos com o tratamento dos dados.\n",
        "\n",
        "| **Nº** | **Coluna**                     | **Tipo**       |\n",
        "|--------|---------------------------------|----------------|\n",
        "| 1      | Apolice Sinistro                | Categórica     |\n",
        "| 2      | Codigo Empresa Sinistro         | Categórica       |\n",
        "| 3      | Nome Empresa Sinistro           | Categórica     |\n",
        "| 4      | SEGURADO                        | Categórica       |\n",
        "| 5      | Codigo Especialidade Sinistro   | Categórica       |\n",
        "| 6      | Elegibilidade Sinistro          | Categórica     |\n",
        "| 7      | Sexo Sinistro                   | Categórica     |\n",
        "| 8      | Faixa-Etária Nova Sinistro      | Categórica     |\n",
        "| 9      | Descricao Plano Sinistro        | Categórica     |\n",
        "| 10     | Codigo Servico Sinistro         | Categórica       |\n",
        "| 11     | Descricao Servico Sinistro      | Categórica     |\n",
        "| 12     | Tipo Utilização Sinistro        | Categórica     |\n",
        "| 13     | Dt Data Sinistro                | Categórica     |\n",
        "| 14     | Codigo Prestador                | Categórica       |\n",
        "| 15     | Nome Prestador Sinistro         | Categórica     |\n",
        "| 16     | Valor Pago Sinistro             | Numérica     |\n",
        "| 17     | Valor Usuario Sinistro          | Numérica       |\n",
        "| 18     | Quantidade                      | Numérica       |\n",
        "| 19     | No Ano Mes                      | Categórica       |\n",
        "| 20     | Codigo Grupo Empresa            | Categórica       |\n",
        "| 21     | Nome Grupo Empresa              | Categórica     |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise Descritiva das Colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Após realizarmos esta análise, podemos observar que a grande maioria das colunas númericas são colunas para identificação. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise Descritiva das Colunas Selecionadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B1P-sUb0o2A"
      },
      "outputs": [],
      "source": [
        "print('Quantidade:')\n",
        "print(df['Quantidade'].describe())\n",
        "\n",
        "df['Valor Pago Sinistro'] = df['Valor Pago Sinistro'].replace(\",\", \".\", regex=True).astype(float)\n",
        "\n",
        "print(\"Valor Pago Sinistro:\")\n",
        "print(f\"min: {df['Valor Pago Sinistro'].min()}\")\n",
        "print(f\"max: {df['Valor Pago Sinistro'].max()}\")\n",
        "print(f\"média: {df['Valor Pago Sinistro'].mean()}\")\n",
        "print(f\"mediana: {df['Valor Pago Sinistro'].median()}\")\n",
        "print(f\"desvio padrão: {df['Valor Pago Sinistro'].std()}\")\n",
        "\n",
        "print('Valor Usuario Sinistro:')\n",
        "print(df['Valor Usuario Sinistro'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P461VYiqAx7F"
      },
      "source": [
        "1. Inicialmente visualizamos as estatísticas descritivas da coluna Quantidade.\n",
        "2. Formatação da coluna Valor Pago Sinistro, pois para realizarmos a análise foi preciso alterar a formatação dos valores que estavam sendo separados por vírgula, alterando para ponto.\n",
        "3. Visualização das estatísticas descritivas da coluna Valor Pago Sinistro.\n",
        "4. Visualização das estatísticas descritivas da coluna Valor Usuario Sinistro, após essa análise podemos observar que essa coluna contém apenas o valor 0 em todas as linas, a tornando uma coluna inutilizável para o treinamento do modelo preditivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gráficos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "colunas_titular = df[df[\"Elegibilidade Sinistro\"] == \"TITULAR\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Nesta primeira etapa criamos uma variável que filtra a base de dados para utilizar apenas linhas que tenhas \"TITULAR\" na coluna Elegibilidade Sinistro. Dessa forma, as análises serão direcionadas para serviços que foram realizados apenas com funcionários da Unipar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gráfico 01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "servico_frequencia = colunas_titular['Descricao Servico Sinistro'].value_counts()\n",
        "\n",
        "top_10_servicos = servico_frequencia.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. A primeira variável conta o número de ocorrências de cada valor na coluna Descricao Servico Sinistro\n",
        "   \n",
        "   <br>\n",
        "\n",
        "    | Descrição do Serviço Ocorrências | Frequência |\n",
        "    |-----------------------------------------------------------------|-------------|\n",
        "    | CONSULTA CONSULTORIO (HORARIO NORMAL OU PREESTAB)               | 4545        |\n",
        "    | SESSAO DE PSICOTERAPIA INDIVIDUAL POR PSICOLOGO                  | 958         |\n",
        "    | HEMOG C/ CONT PLAQ OU FCS (ERITROGR, LEUCOG, PLAQ)               | 908         |\n",
        "    | GLICOSE - PESQUISA E/OU DOSAGEM                                  | 797         |\n",
        "    | CREATININA - PESQUISA E/OU DOSAGEM                               | 793         |\n",
        "    | ...                                                             | ...         |\n",
        "    | PROCEDIMENTO PADRONIZADO ANGIO-TC                                | 1           |\n",
        "    | URETEROTOMIA INTERNA URETEROSCOPICA FLEXIVEL UNILATERAL          | 1           |\n",
        "    | CORPO ESTRANHO - EXTRACAO ENDOSCOPICA                            | 1           |\n",
        "    | PP HERPES SIMPLES - IGG - PESQUISA E/OU DOSAGEM                  | 1           |\n",
        "    | AVAL. CLIN E ELETR. PCT PORT. MARCA-PAS/SINCR/DESF               | 1           |\n",
        "\n",
        "<br>\n",
        "\n",
        "3. Em seguida são selecionados os 10 serviços mais frequentes na coluna 'Descricao Servico Sinistro' e armazena na variável 'top_10_servicos'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "top_10_servicos.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Top 10 Serviços Mais Utilizados')\n",
        "plt.xlabel('Código do Serviço Sinistro')\n",
        "plt.ylabel('Frequência de Uso')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Criação de um gráfico para ilustrar a análise realizada com os dados. Um gráfico de barras será utilizado para visualizar os top 10 serviços mais utilizados pelos funcionários da Unipar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gráfico 02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "idade_frequencia = colunas_titular['Faixa-Etária Nova Sinistro'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Variável que conta o número de ocorrências de cada valor na coluna Faixa-Etária Nova Sinistro.\n",
        "\n",
        "    <br>\n",
        "\n",
        "    | Faixa-Etária Nova Sinistro | Frequência |\n",
        "    |-------------------|---------------|\n",
        "    | 59 anos ou mais   | 9742          |\n",
        "    | 34 a 38 anos      | 6480          |\n",
        "    | 39 a 43 anos      | 5293          |\n",
        "    | 44 a 48 anos      | 4371          |\n",
        "    | 49 a 53 anos      | 3751          |\n",
        "    | 54 a 58 anos      | 3558          |\n",
        "    | 29 a 33 anos      | 3185          |\n",
        "    | 24 a 28 anos      | 2067          |\n",
        "    | 19 a 23 anos      | 1062          |\n",
        "    | 0 a 18 anos       | 14            |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "idade_frequencia.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Faixa-Etária Colaboradores')\n",
        "plt.xlabel('Faixas de idade')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Criação de um gráfico para ilustrar a análise realizada com os dados. Um gráfico de barras será utilizado para visualizar a distribuição do número de sinistros por faixa etária.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gráfico 03\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plano_frequencia = colunas_titular['Descricao Plano Sinistro'].value_counts()\n",
        "print(plano_frequencia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Variável que conta o número de ocorrências de cada valor na coluna Descricao Plano Sinistro.\n",
        "\n",
        "    <br>\n",
        "\n",
        "    | Descrição do Plano Ocorrências | Frequência |\n",
        "    |--------------------|-------------|\n",
        "    | TQN2               | 17697       |\n",
        "    | NP2X               | 2776        |\n",
        "    | NP6X               | 1515        |\n",
        "    | TNQ2               | 1223        |\n",
        "    | TN1E               | 1117        |\n",
        "    | TP8X               | 619         |\n",
        "    | TNE1               | 437         |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "plano_frequencia.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Planos Colaboradores')\n",
        "plt.xlabel('Planos')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Criação de um gráfico para ilustrar a análise realizada com os dados. Um gráfico de barras será utilizado para visualizar quais planos de saúde são mais utilizados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, vamos renomear todas as colunas para seguir as boas práticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.rename(columns={'Apolice Sinistro': 'apolice_sinistro','Codigo Empresa Sinistro': 'codigo_empresa_sinistro','Nome Empresa Sinistro': 'nome_empresa_sinistro','SEGURADO': 'segurado','Codigo Especialidade Sinistro': 'codigo_especialidade_sinistro','Elegibilidade Sinistro': 'elegibilidade_sinitro','Sexo Sinistro': 'sexo_sinistro','Faixa-Etária Nova Sinistro': 'faixa_etaria_sinistro','Descricao Plano Sinistro': 'descricao_plano_sinistro','Codigo Servico Sinistro': 'codigo_servico_sinistro','Descricao Servico Sinistro': 'descricao_servico_sinistro','Tipo Utilização Sinistro': 'tipo_utilizacao_sinistro','Dt Data Sinistro': 'data_sinistro','Codigo Prestador': 'codigo_prestador','Nome Prestador Sinistro': 'nome_prestador_sinistro','Valor Pago Sinistro': 'valor_pago_sinistro','Valor Usuario Sinistro': 'valor_usuario_sinistro','Quantidade': 'quantidade','No Ano Mes': 'ano_mes','Codigo Grupo Empresa': 'codigo_grupo_empresa','Nome Grupo Empresa': 'nome_grupo_empresa',})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, precisamos filtrar nossa base de dados para conter apenas os Titulares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df['elegibilidade_sinitro'] == 'TITULAR']\n",
        "df['elegibilidade_sinitro'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None) # Configuracao para mostrar todas as colunas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tratamento de valores duplicados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazer a limpeza de valores duplicados é importantes pois valores duplicados podem prejudicar no aprendizado do nosso algoritmo. Para realizar a limpeza, vamos primeiro checar se existem valores duplicados e caso existirem, vamos excluir todos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicados = df[df.duplicated()]\n",
        "print(len(duplicados))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, existem 130 valores duplicados, o que vamos fazer é excluir todos para que assim eles não atrapalhem nosso algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tratamento de missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O comando df.isnull().sum() encontra todos os missing values. E como podemos ver, não existe nenhum valor null na nossa base de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Também ao olhar a coluna 'nome_prestador_sinistro', vemos que existem 3792 casos em que o valor é igual = \"NAO INFORMADO\". Como não podemos descobrir qual é o nome do prestador, vamos deixar do jeito que está."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['nome_prestador_sinistro'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Já esse comando é capaz de encontrar todos os valores = 0. Como podemos ver, Existem alguns valores importantes iguais a 0 e que não poderiam estar assim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_count = (df == 0).sum()\n",
        "print(zero_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para resolver os valores 0 na quantidade, vamos substituir eles pela moda da coluna, que é 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['quantidade'] = df['quantidade'].replace(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazendo uma outra análise, podemos ver que UNIPAR INDUPA DO BRASIL S.A e UNIPAR INDUPA DO BRASIL S.A. são a mesma empresa mas elas se diferem pelo ponto. Provavelmente um erro de digitação, esse é um erro que precisa ser tratado!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['nome_empresa_sinistro'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['nome_empresa_sinistro'] = df['nome_empresa_sinistro'].replace('UNIPAR INDUPA DO BRASIL S.A.', 'UNIPAR INDUPA DO BRASIL S.A')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['nome_empresa_sinistro'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identificação de outliers e correção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As três colunas com possíveis outliers, são \"valor_pago_sinistro\", \"valor_usuario_sinistro\" e \"quantidade\". Isso porque todas as outras colunas numéricas são de identificação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, a coluna \"valor_usuario_sinistro\" não possui outliers, já que contém apenas o valor 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['valor_usuario_sinistro'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(data=df, y='valor_pago_sinistro')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(data=df, y='quantidade')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, as duas colunas acima, possuem incontáveis outliers. Agora vamos tratá-los.\n",
        "\n",
        "Basta observar os gráficos acima para entender que apesar de existirem milhares de outliers, muitos deles acontecem na mesma faixa e outros são casos raros, mesmo assim são números muito grandes. Por isso, vamos remover os valores em 'valor_pago_sinistro' que são maiores que 10.000 e os valores maiores que 10 em 'quantidade'.\n",
        "\n",
        "Os valores maiores que 10.000 em 'valor_pago_sinistro' somam apenas 275 ocorrências. São poucas ocorrências e por isso vamor excluir elas, caso contrário, vão atrapalhar nosso algoritmo pois são outliers muito significativos.\n",
        "\n",
        "Já os valores maiores que 10 em 'quantidade', somam apenas 9 ocorrências, pelos mesmos motivos, vamos excluir cada uma.\n",
        "\n",
        "Para tratar desse problema, vamos criar uma cópia do nosso banco de dados e excluir esses valores do nosso novo banco de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_no_outliers = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_no_outliers = df_no_outliers[df_no_outliers['valor_pago_sinistro'] < 10000]\n",
        "df_no_outliers = df_no_outliers[df_no_outliers['quantidade'] < 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(data=df_no_outliers, y='valor_pago_sinistro')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.boxplot(data=df_no_outliers, y='quantidade')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar, ainda existem outliers, mas esses são valores que aconteceram muito e por isso possuem a tendência de acontecer novamente, logo, não vale a pena excluir esses valores da nossa base de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Também é importante excluir as colunas que possuem apenas um único valor em todas as linhas porque assim vamos garantir vários benefícios, por exemplo:\n",
        "\n",
        "1. Redução de Redundância\n",
        "2. Simplificação do Modelo\n",
        "3. Melhoria do Desempenho do Modelo\n",
        "4. Redução de Overfitting\n",
        "5. Economia de Espaço e Tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_with_unique_values = [col for col in df.columns if df[col].nunique() == 1]\n",
        "print(columns_with_unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned = df_no_outliers.drop(columns=columns_with_unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Codificação correta das variáveis de data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uma coluna que será importante para nosso e que precisa ser tratada, é a coluna 'data_sinistro'. Como ela é uma coluna que contém datas, primeiro precisamos transformar os valores em *timestamp*, que é uma representação numérica que indica um ponto específico no tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para isso, vamos usar o comando abaixo, ele transformará nossa coluna para o padrão que desejamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned['data_sinistro'] = pd.to_datetime(df['data_sinistro'], format='%d/%m/%Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como nossos componentes temporais são relevantes para a análise, precisamos extrair o ano, mês e dia da nossa coluna para facilitar a identificação de sazionalidades e tendências anuais.\n",
        "\n",
        "Após extrair os componentes temporais, vamos excluir a coluna 'data_sinistro' pois ela já não é mais necessária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned['dia_data_sinistro'] = df_cleaned['data_sinistro'].dt.day\n",
        "df_cleaned['mes_data_sinistro'] = df_cleaned['data_sinistro'].dt.month\n",
        "df_cleaned['ano_data_sinistro'] = df_cleaned['data_sinistro'].dt.year\n",
        "df_cleaned = df_cleaned.drop(columns=['data_sinistro'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, vamos realizar a exclusão da coluna 'ano_mes' da nossa base de dados, já que não vamos utilizar ela para o treinamento do nosso modelo. Isso porque vemos como mais importante identificar sazonalidades no momento em que o segurado utilizou o plano de saúde, isso é mostrado nas colunas 'dia_data_sinistro', 'mes_data_sinistro' e 'ano_data_sinistro'. Já a coluna 'ano_mes', mostra apenas quando é lançada a fatura do sinistro para a empresa. Por essas razões, vamos excluir essa coluna. \n",
        "\n",
        "Duas colunas que também não serão utilizadas são: 'codigo_prestador' e 'nome_prestador_sinistro'. Essas colunas apenas identificam o prestador do sinistro e, portanto, não são relevantes para nossa análise, que visa encontrar insights relacionados aos sinistros em si."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned = df_cleaned.drop(columns=['ano_mes', 'codigo_prestador', 'nome_prestador_sinistro'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Codificação correta das variáveis categóricas e normalização das variáveis numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para codificar corretamente as variáveis categóricas, primeiro, vamos colocar em uma array todas as colunas com variáveis categóricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "object_columns = [col for col in df_cleaned.columns if df_cleaned[col].dtype == 'object']\n",
        "print(object_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in object_columns:\n",
        "    print(df_cleaned[col].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver acima, 'nome_empresa_sinistro' e 'tipo_utilizacao_sinistro' possuem apenas 2 valores únicos. Como queremos que o algoritmo diferencie com clareza as empresas e tipos de utilização do sinistro, vamos utilizar o oneHotEncoder para codificar corretamente essas duas variáveis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "nome_empresa_sinistro_encoded_array = hot_encoder.fit_transform(df_cleaned[['nome_empresa_sinistro']])\n",
        "nome_empresa_sinistro_encoded_dataframe = pd.DataFrame(nome_empresa_sinistro_encoded_array, columns=hot_encoder.get_feature_names_out(['nome_empresa_sinistro']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "tipo_utilizacao_sinistro_encoded_array = hot_encoder.fit_transform(df_cleaned[['tipo_utilizacao_sinistro']])\n",
        "tipo_utilizacao_sinistro_encoded_dataframe = pd.DataFrame(tipo_utilizacao_sinistro_encoded_array, columns=hot_encoder.get_feature_names_out(['tipo_utilizacao_sinistro']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cleaned.reset_index(drop=True, inplace=True)\n",
        "nome_empresa_sinistro_encoded_dataframe.reset_index(drop=True, inplace=True)\n",
        "tipo_utilizacao_sinistro_encoded_dataframe.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_encoded = pd.concat([df_cleaned, nome_empresa_sinistro_encoded_dataframe, tipo_utilizacao_sinistro_encoded_dataframe], axis=1)\n",
        "df_encoded = df_encoded.drop(columns=['nome_empresa_sinistro', 'tipo_utilizacao_sinistro'])\n",
        "df_encoded = df_encoded.rename(columns={'nome_empresa_sinistro_UNIPAR CARBOCLORO S.A': 'nome_empresa_sinistro_unipar_carbocloro_s.a', 'nome_empresa_sinistro_UNIPAR INDUPA DO BRASIL S.A': 'nome_empresa_sinistro_unipar_indupa_do_brasil_s.a', 'tipo_utilizacao_sinistro_REDE': 'tipo_utilizacao_sinistro_rede', 'tipo_utilizacao_sinistro_REEMBOLSO': 'tipo_utilizacao_sinistro_reembolso',})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para terminar a codificação das variáveis categóricas, vamos utilizar o labelEncoder() que converte variáveis categóricas em variáveis numéricas. Pra isso, vamos criar uma array com as últimas colunas com variáveis categóricas e transformar elas com o labelEncoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "last_object_columns = [col for col in df_encoded.columns if df_encoded[col].dtype == 'object']\n",
        "print(last_object_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in last_object_columns:\n",
        "    df_encoded[col] = label_encoder.fit_transform(df_encoded[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora para fazer a normalização das variáveis numéricas, as únicas colunas disponíveis para fazer isso seriam as colunas 'valor_pago_sinistro', 'quantidade', 'dia_data_sinistro', 'mes_data_sinistro', 'ano_data_sinistro' e 'codigo_servico_sinistro'.\n",
        "\n",
        "Não vamos realizar a normalização das outras colunas numéricas porque já utilizamos o LabelEncoder ou OneHotEncoder.\n",
        "\n",
        "É importante normalizar as variáveis numéricas já que muitos algoritmos são sensíveis à escala dos dados.\n",
        "\n",
        "Para fazer a normalização das variáveis numéricas, vamos usar o StandardScaler, já que esse método é menos sensível à outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "valor_pago_sinistro_esc = scaler.fit_transform(df_cleaned[['valor_pago_sinistro']])\n",
        "quantidade_esc = scaler.fit_transform(df_cleaned[['quantidade']])\n",
        "dia_data_sinistro_esc = scaler.fit_transform(df_cleaned[['dia_data_sinistro']])\n",
        "mes_data_sinistro_esc = scaler.fit_transform(df_cleaned[['mes_data_sinistro']])\n",
        "ano_data_sinistro_esc = scaler.fit_transform(df_cleaned[['ano_data_sinistro']])\n",
        "codigo_servico_sinistro_esc = scaler.fit_transform(df_cleaned[['codigo_servico_sinistro']])\n",
        "\n",
        "valor_pago_sinistro_esc_dataframe = pd.DataFrame(valor_pago_sinistro_esc, columns=['valor_pago_sinistro_esc'])\n",
        "quantidade_esc_dataframe = pd.DataFrame(quantidade_esc, columns=['quantidade_esc'])\n",
        "dia_data_sinistro_esc_dataframe = pd.DataFrame(dia_data_sinistro_esc, columns=['dia_data_sinistro_esc'])\n",
        "mes_data_sinistro_esc_dataframe = pd.DataFrame(mes_data_sinistro_esc, columns=['mes_data_sinistro_esc'])\n",
        "ano_data_sinistro_esc_dataframe = pd.DataFrame(ano_data_sinistro_esc, columns=['ano_data_sinistro_esc'])\n",
        "codigo_servico_sinistro_esc_dataframe = pd.DataFrame(codigo_servico_sinistro_esc, columns=['codigo_servico_sinistro_esc'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded_esc = pd.concat([df_encoded, valor_pago_sinistro_esc_dataframe, quantidade_esc_dataframe, dia_data_sinistro_esc_dataframe, mes_data_sinistro_esc_dataframe, ano_data_sinistro_esc_dataframe, codigo_servico_sinistro_esc_dataframe], axis=1)\n",
        "df_encoded_esc = df_encoded_esc.drop(columns=['valor_pago_sinistro', 'quantidade', 'dia_data_sinistro', 'mes_data_sinistro', 'ano_data_sinistro', 'codigo_servico_sinistro'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded_esc.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hipóteses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hipótese 1: A faixa etária dos colaboradores pode ter uma correlação positiva com a sinistralidade do plano de saúde\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; É notável que, na maioria dos casos, pessoas em faixas etárias mais avançadas tendem a ter maiores necessidades médicas, resultando em um uso mais frequente do plano de saúde devido a consultas e visitas hospitalares. Caso os dados realmente demonstrem que colaboradores com idade mais avançada estão utilizando mais o plano de saúde, seria interessante considerar a implementação de um plano especializado, focado na prevenção e no tratamento de doenças crônicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agrupar por Faixa Etária e calcular o valor médio dos sinistros\n",
        "media_sinistro_faixa_etaria = df.groupby('faixa_etaria_sinistro')['valor_pago_sinistro'].sum().reset_index()\n",
        "\n",
        "soma_do_sinistro = media_sinistro_faixa_etaria.head(10)\n",
        "\n",
        "media_sinistro_faixa_etaria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "soma_do_sinistro.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Faixa Etária Comparada com Valor Pago de Sinistro')\n",
        "plt.xlabel('Faixa Etária Sinistro')\n",
        "plt.ylabel('Valor Pago Sinistro')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp; Diante dessa análise dos dados, concluímos que a faixa etária dos colaboradores, em conjunto com o valor pago pelos sinistros em cada tipo de consulta, indica que esses sinistros estão sendo utilizados predominantemente por colaboradores de faixa etária mais elevada dentro da empresa. Com isso, a UNIPAR pode considerar a criação de planos de apoio mais específicos para esses colaboradores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hipótese 2: O exame de Psicoterapia Individual por Psicólogo apresenta uma alta taxa de utilização por parte dos colaboradores.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;Nos últimos anos, as empresas têm se preocupado cada vez mais com a saúde mental de seus funcionários, conscientizando-os sobre a importância desse aspecto, o que pode ter um impacto significativo no ambiente corporativo. Dessa forma, é esperado que a taxa de utilização de consultas psicológicas seja alta entre os colaboradores. Essa alta demanda pode indicar que as necessidades de saúde mental dos funcionários estão sendo reconhecidas e devidamente atendidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "servico_frequencia = colunas_titular['Descricao Servico Sinistro'].value_counts()\n",
        "\n",
        "top_10_servicos= servico_frequencia.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "top_10_servicos.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Top 10 Serviços Mais Utilizados')\n",
        "plt.xlabel('Descricao Serviço Sinistro')\n",
        "plt.ylabel('Frequência de Uso')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Diante esse gráfico podemos ver que o segundo tipo de serviço de sinistro mais utilizado dentro da empresa UNIPAR é a sessão de psicoterapia, e com isso comprovamos nossa hipótese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hipótese 3: O plano de sinistro mais utilizado pelos colaboradores foi o TQN2.\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;O plano TQN2 pode oferecer uma cobertura abrangente ou benefícios que são mais atrativos ou relevantes para os colaboradores em comparação com outros planos disponíveis pela empresa. Isso poderia incluir uma melhor cobertura para consultas, exames, terapias, ou uma rede credenciada mais extensa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "sinistro_frequencia = colunas_titular['Descricao Plano Sinistro'].value_counts()\n",
        "\n",
        "top_10_descricoes= sinistro_frequencia.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotar o gráfico de barras\n",
        "top_10_descricoes.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Adicionar título e rótulos aos eixos\n",
        "plt.title('Top 10 Planos de Sinistro Mais Utilizados')\n",
        "plt.xlabel('Descricao Plano Sinistro')\n",
        "plt.ylabel('Frequência de Uso')\n",
        "\n",
        "# Exibir o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Com esse gráfico, podemos analisar que é inevitável a diferença de quantidade de uso do plano TQN2, com isso podemos ainda mais aumentar a eficiência dessa nossa hipótese criada\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hipótese 4: A faixa etária dos colaboradores que mais acionam o plano de Sinistro TQN2 é 34 a 38 anos\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;A faixa etária dos colaboradores que mais acionam o plano de Sinistro TQN2 é de 34 a 38 anos. Essa hipótese baseia-se na ideia de que, nessa fase da vida, os colaboradores podem estar enfrentando um aumento nas necessidades de saúde. A confirmação ou refutação dessa hipótese será explorada na análise do gráfico a seguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfTqn2 = df[df[\"descricao_plano_sinistro\"] == \"TQN2\"]\n",
        "tqn2PorFaixaEtariaSexo = dfTqn2.groupby(['faixa_etaria_sinistro', 'sexo_sinistro']).size().unstack(fill_value=0)\n",
        "\n",
        "faixasEtarias = tqn2PorFaixaEtariaSexo.index\n",
        "sexCounts = {\n",
        "    'Masculino': tqn2PorFaixaEtariaSexo['M'].values,\n",
        "    'Feminino': tqn2PorFaixaEtariaSexo['F'].values,\n",
        "}\n",
        "width = 0.6 \n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "bottom = np.zeros(len(faixasEtarias))\n",
        "\n",
        "for sexo, counts in sexCounts.items():\n",
        "    p = ax.bar(faixasEtarias, counts, width, label=sexo, bottom=bottom)\n",
        "    bottom += counts\n",
        "\n",
        "    ax.bar_label(p, label_type='center')\n",
        "\n",
        "ax.set_title('Utilização do Plano TQN2 por Faixa Etária e Sexo')\n",
        "ax.set_xlabel('Faixa Etária')\n",
        "ax.set_ylabel('Quantidade de Sinistros')\n",
        "\n",
        "ax.legend(title='Sexo')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Com a análise do gráfico acima, podemos observar que dentro do plano mais utilizado os colaboradores da faixa etária de 34 a 38 anos acionam mais sinistros deste plano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hipótese 5: Existe uma tendência maior dos colaboradores da faixa etária de 59 anos ou mais acionarem mais o sinistro de Consulta no Consultório\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;Existe uma tendência maior dos colaboradores na faixa etária de 59 anos ou mais de acionar o sinistro de Consulta no Consultório com maior frequência. Essa hipotese baseia-se na ideia de que, com o avanço da idade, as necessidades de cuidados médicos regulares e acompanhamento contínuo aumentam, fazendo com que essa faixa etária busque mais frequentemente consultas médicas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_faixa59ouMais = df[df[\"faixa_etaria_sinistro\"] == \"59 anos ou mais\"]\n",
        "frequenciaSinistroFaixa59ouMais = df_faixa59ouMais['descricao_servico_sinistro'].value_counts()\n",
        "sinistrosFaixa59ouMais = frequenciaSinistroFaixa59ouMais.head(5)\n",
        "\n",
        "#Plotar o gráfico\n",
        "sinistrosFaixa59ouMais.plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'lightgreen', 'lightcoral', 'orange', 'lightpink'])\n",
        "plt.title('5 Serviços mais utilizados pelos colaboradores da faixa etária 59 anos ou mais')\n",
        "\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Após realizarmos a nossa analise gráfica podemos observar que dentre os serviços de sinistro mais utilizado para esta faixa etária é a CONSULTA CONSULTORIO (HORARIO NORMAL OU PRESTAB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hipótese 6: Existe uma tendência maior dos colaboradores da faixa etária de 59 anos ou mais acionarem mais o sinistro de Consulta no Consultório durante o mês de junho\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;Há uma tendência maior dos colaboradores da faixa etária de 59 anos ou mais de acionarem o sinistro de Consulta no Consultório especificamente durante o mês de junho. Essa hipótese está relacionada a fatores sazonais, como mudanças climáticas durante este mês."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_faixa59ouMais = df[df[\"faixa_etaria_sinistro\"] == \"59 anos ou mais\"]\n",
        "df_faixa59ouMais = df_faixa59ouMais[df_faixa59ouMais[\"descricao_servico_sinistro\"] == \"CONSULTA CONSULTORIO (HORARIO NORMAL OU PREESTAB)\"]\n",
        "\n",
        "df_faixa59ouMais['data_sinistro'] = pd.to_datetime(df_faixa59ouMais['data_sinistro'], format='%d/%m/%Y')\n",
        "df_2023 = df_faixa59ouMais[df_faixa59ouMais['data_sinistro'].dt.year == 2023]\n",
        "sinistros_por_mes_2023 = df_2023['data_sinistro'].dt.month.value_counts().sort_index()\n",
        "\n",
        "sinistros_por_mes_2023_df = sinistros_por_mes_2023.reset_index()\n",
        "sinistros_por_mes_2023_df.columns = ['Mês', 'Quantidade de Sinistros']\n",
        "\n",
        "# Plotar o gráfico\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sinistros_por_mes_2023_df['Mês'], sinistros_por_mes_2023_df['Quantidade de Sinistros'], marker='o', linestyle='-', color='blue')\n",
        "\n",
        "plt.title('Tendência de Sinistros por Mês em 2023')\n",
        "plt.xlabel('Mês')\n",
        "plt.ylabel('Quantidade de Sinistros')\n",
        "plt.xticks(ticks=range(1, 13), labels=['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', 'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez'])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;Realizamos uma análise da quantidade de acionamentos do serviço de sinistro Consulta no Consultório durante todos os meses de 2023, muito importante enfatizarmos que está análise foi realizada de acordo com as datas dos dados disponibilizados, portanto não é possível realizarmos uma anáise mais específica da sazonalidade ao passar dos anos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Caso Não-Supervisionado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelagem, features e explicação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelagem do problema e clareza na explicação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O objetivo principal do nosso modelo é analisar e agrupar os dados fornecidos pela Unipar, utilizando técnicas de machine learning, especificamente clustering, para extrair insights importantes sobre os padrões de utilização do plano de saúde pelos colaboradores. Estamos utilizando aprendizagem não supervisionada, mais precisamente clustering, pelas seguintes razões:\n",
        "\n",
        "1) Não temos categorias predefinidas de problemas de saúde ou padrões de utilização, tornando a aprendizagem não supervisionada a escolha mais apropriada.\n",
        "2) Queremos descobrir padrões naturais nos dados de saúde sem impor categorias predefinidas, permitindo uma visão mais abrangente e potencialmente reveladora.\n",
        "3) O clustering nos permite agrupar colaboradores com perfis de saúde e utilização do plano similares, revelando estruturas intrínsecas que podem não ser imediatamente aparentes nos relatórios convencionais.\n",
        "\n",
        "O algoritmo de clustering que estamos empregando, vai nos ajudar a segmentar os dados em grupos distintos baseados em características similares de utilização do plano de saúde e perfis de saúde dos colaboradores.\n",
        "\n",
        "Esperamos que este modelo resolva o problema em questão fornecendo um agrupamento capaz de revelar insights valiosos sobre os padrões e tendências nos dados de saúde. Com esses insights, a Unipar será capaz de:\n",
        "\n",
        "1) Identificar segmentos distintos de colaboradores com necessidades de saúde similares.\n",
        "2) Entender as características comuns dentro de cada grupo de saúde.\n",
        "3) Desenvolver programas de saúde personalizados para cada segmento identificado.\n",
        "4) Otimizar a alocação de recursos e a eficácia dos programas de saúde com base nas particularidades de cada grupo.\n",
        "\n",
        "Por exemplo, o modelo pode revelar grupos de colaboradores com padrões similares de utilização do plano de saúde ou com condições de saúde semelhantes, permitindo que a Unipar desenvolva programas de saúde mais direcionados e eficazes, como programas para doenças crônicas, tabagismo, alcoolismo ou programas para gestantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Escolha das features e justificativas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As features que escolhemos incluir em nosso modelo de clustering são:\n",
        "\n",
        "'codigo_empresa_sinistro', 'codigo_especialidade_sinistro', 'sexo_sinistro', 'faixa_etaria_sinistro', 'descricao_plano_sinistro', 'descricao_servico_sinistro', 'nome_empresa_sinistro_unipar_carbocloro_s.a', 'nome_empresa_sinistro_unipar_indupa_do_brasil_s.a', 'tipo_utilizacao_sinistro_rede', 'tipo_utilizacao_sinistro_reembolso', 'valor_pago_sinistro_esc', 'quantidade_esc', 'dia_data_sinistro_esc', 'mes_data_sinistro_esc', 'ano_data_sinistro_esc', 'codigo_servico_sinistro_esc'\n",
        "\n",
        "Justificativa para a escolha destas features:\n",
        "\n",
        "1. Relevância para o problema: \n",
        "   Decidimos manter praticamente todas as colunas disponibilizadas pela Unipar porque cada uma delas fornece informações valiosas sobre os padrões de utilização do plano de saúde e as características dos colaboradores. Isso é crucial para nosso objetivo de criar programas de saúde personalizados e eficazes.\n",
        "\n",
        "   - 'codigo_empresa_sinistro', 'nome_empresa_sinistro_unipar_carbocloro_s.a', 'nome_empresa_sinistro_unipar_indupa_do_brasil_s.a': \n",
        "   \n",
        "      Estas features nos permitem identificar padrões específicos de cada empresa dentro do grupo Unipar.\n",
        "   \n",
        "   - 'codigo_especialidade_sinistro', 'descricao_servico_sinistro', 'codigo_servico_sinistro_esc': \n",
        "   \n",
        "      Estas nos ajudam a entender quais tipos de serviços médicos são mais utilizados.\n",
        "   \n",
        "   - 'sexo_sinistro', 'faixa_etaria_sinistro': \n",
        "   \n",
        "      Fundamentais para identificar padrões demográficos de saúde.\n",
        "   \n",
        "   - 'descricao_plano_sinistro': \n",
        "   \n",
        "      Nos permite analisar se há diferenças significativas na utilização entre diferentes planos.\n",
        "   \n",
        "   - 'tipo_utilizacao_sinistro_rede', 'tipo_utilizacao_sinistro_reembolso': \n",
        "   \n",
        "      Ajudam a entender como os colaboradores estão utilizando o plano (dentro ou fora da rede).\n",
        "   \n",
        "   - 'valor_pago_sinistro_esc', 'quantidade_esc': \n",
        "   \n",
        "      Importantes para identificar padrões de custo e frequência de utilização.\n",
        "   \n",
        "   - 'dia_data_sinistro_esc', 'mes_data_sinistro_esc', 'ano_data_sinistro_esc': \n",
        "   \n",
        "      Permitem identificar padrões sazonais ou tendências ao longo do tempo.\n",
        "\n",
        "2. Pré-processamento e transformação:\n",
        "   Para otimizar o desempenho do nosso algoritmo de clustering, aplicamos o seguinte pré-processamento:\n",
        "   - Escalonamento: Utilizamos o StandardScaler para escalonar toda nossa base de dados. Isso é crucial para garantir que todas as features contribuam igualmente para o clustering, independentemente de suas escalas originais.\n",
        "\n",
        "3. Adequação para aprendizagem não supervisionada:\n",
        "   Estas features são adequadas para um modelo de aprendizagem não supervisionada, especificamente clustering, porque:\n",
        "   \n",
        "   a) Diversidade de informações: As features selecionadas cobrem uma ampla gama de aspectos relacionados à utilização do plano de saúde, desde características demográficas até detalhes específicos dos serviços utilizados.\n",
        "   \n",
        "   b) Potencial para revelar padrões ocultos: Ao combinar informações sobre o tipo de serviço, custo, frequência e características do usuário, nosso modelo pode identificar grupos de colaboradores com necessidades de saúde similares que não seriam evidentes em uma análise tradicional.\n",
        "   \n",
        "   c) Ausência de rótulos predefinidos: Como não temos categorias predefinidas de \"perfis de saúde\", a abordagem não supervisionada nos permite descobrir naturalmente esses perfis a partir dos dados.\n",
        "   \n",
        "   d) Escalabilidade: Ao escalonar os dados, garantimos que o algoritmo de clustering possa operar eficientemente, considerando todas as features de maneira equitativa.\n",
        "\n",
        "Ao utilizar estas features em nosso modelo de clustering, esperamos identificar grupos significativos de colaboradores com padrões similares de utilização do plano de saúde. Isso permitirá que a Unipar desenvolva programas de saúde mais direcionados e eficazes, potencialmente melhorando a saúde dos colaboradores e otimizando os custos relacionados ao plano de saúde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modelo e discussão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apresentar o modelo candidato "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O modelo candidato que escolhemos para este projeto é o K-Means, um algoritmo de clustering amplamente utilizado em aprendizagem não supervisionada. O K-Means é particularmente adequado para nosso caso devido à sua eficácia em lidar com grandes conjuntos de dados e sua capacidade de identificar padrões em dados multidimensionais, como os que temos em nossa base de utilização do plano de saúde.\n",
        "\n",
        "Aqui, cito uma explicação do funcionamento do K-Means:\n",
        "\n",
        "- Inicialização: O algoritmo começa escolhendo aleatoriamente K pontos no espaço de dados como centróides iniciais, onde K é o número de clusters que desejamos formar.\n",
        "- Atribuição: Cada ponto de dados é atribuído ao centróide mais próximo, formando K clusters iniciais.\n",
        "- Atualização: O centróide de cada cluster é recalculado como a média de todos os pontos atribuídos a ele.\n",
        "- Repetição: Os passos 2 e 3 são repetidos iterativamente até que os centróides se estabilizem ou um número máximo de iterações seja atingido.\n",
        "- Convergência: O algoritmo converge quando as atribuições não mudam mais.\n",
        "\n",
        "Por essas razões, escolhemos o K-Means como nosso primeiro modelo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, vamos começar o treinamento do nosso primeiro modelo com o K-Means.\n",
        "\n",
        "Primeiro, vamos criar uma base de dados chamada 'df_encoded_all_esc' para o treinamento do nosso modelo. Também, vamos excluir a coluna 'segurado' da nossa base de dados porque essa coluna é apenas uma forma de identificar os titulares, logo, essa informação não é importante para o treinamento do nosso algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded_all_esc = df_encoded_esc.drop(columns=['segurado'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vamos trabalhar com o K-Means, a distância entre cada dado é extremamente importante para o agrupamento e formação dos grupos. Por isso, vamos utilizar o StandardScaler para escalonar os dados e consequentemente diminuir a distância entre cada ponto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_encoded_all_esc = scaler.fit_transform(df_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definição do K e justificativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para aplicar o algoritmo K-Means, precisamos definir o valor de K, que representa o número de clusters em que queremos dividir nossos dados. O K é um hiperparâmetro crucial que determina quantos grupos distintos o algoritmo deve identificar nos dados de utilização do plano de saúde da Unipar.\n",
        "\n",
        "A escolha do valor ideal de K é fundamental, pois influencia diretamente a qualidade e a interpretabilidade dos resultados do clustering. Um K muito baixo pode resultar em clusters muito amplos e genéricos, perdendo nuances importantes nos padrões de utilização do plano de saúde. Por outro lado, um K muito alto pode levar a clusters excessivamente específicos, dificultando a identificação de padrões significativos e aumentando o risco de overfitting.\n",
        "Para definir com precisão o número de clusters, vamos utilizar o Elbow Method. Este método é uma técnica heurística comum para determinar o número ideal de clusters (K) em um conjunto de dados.\n",
        "\n",
        "O Elbow Method funciona da seguinte maneira:\n",
        "\n",
        "1. Executamos o algoritmo K-Means para diferentes valores de K (por exemplo, de 1 a 10).\n",
        "2. Para cada K, calculamos a soma das distâncias quadradas dentro do cluster (WCSS - Within-Cluster Sum of Squares).\n",
        "3. Plotamos um gráfico de WCSS versus K.\n",
        "4. Procuramos o \"cotovelo\" no gráfico - o ponto onde a diminuição da WCSS começa a nivelar, formando uma curva semelhante a um cotovelo.\n",
        "O valor de K neste ponto de \"cotovelo\" é considerado o número ideal de clusters, pois representa um equilíbrio entre a compactação do cluster e o número de clusters.\n",
        "\n",
        "Este método nos ajuda a encontrar um ponto de equilíbrio onde adicionar mais clusters não melhora significativamente a qualidade do agrupamento, permitindo-nos escolher um número de clusters que seja tanto eficaz quanto computacionalmente eficiente para nossa análise dos dados de saúde da Unipar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "elbow_method_all_encoded = []\n",
        "for i in range(1,11):\n",
        "\tkmeans = KMeans(n_clusters=i,init='k-means++',random_state=42,max_iter=300)\n",
        "\tkmeans.fit(df_encoded_all_esc)\n",
        "\telbow_method_all_encoded.append(kmeans.inertia_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(x=range(1,11),y=elbow_method_all_encoded,marker='o',color='red')\n",
        "plt.title('Number of Clusters (K)')\n",
        "plt.xlabel('Inertia')\n",
        "plt.ylabel('Elbow Method for Optimal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observando o gráfico, é possível observar que o ponto onde a diminuição da WCSS começa a nivelar, formando uma curva semelhante a um cotovelo se encontra no valor 4. Com essa informação, é possível concluir que o número ideal de Clusters (K) é 4, logo, vamos trabalhar com esse valor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com o código abaixo, vamos treinar nosso primeiro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans_esc_all = KMeans(n_clusters=4,init='k-means++',random_state=42,max_iter=300)\n",
        "all_esc_kmeans = kmeans_esc_all.fit(df_encoded_all_esc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = all_esc_kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_esc = PCA(n_components=2)\n",
        "reduced_data = pca_esc.fit_transform(df_encoded_all_esc)\n",
        "\n",
        "explained_variance_ratio = pca_esc.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "print(f\"Variance explained by the first two principal components: {cumulative_variance_ratio[1]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para visualizar com qualidade como o agrupamento foi feito, criamos um gráfico em 2D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], \n",
        "                      c=labels, cmap='viridis')\n",
        "\n",
        "plt.xlabel(f'First PC ({explained_variance_ratio[0]:.2%} variance)')\n",
        "plt.ylabel(f'Second PC ({explained_variance_ratio[1]:.2%} variance)')\n",
        "plt.title('K-means Clustering (4 clusters) - 2D PCA Visualization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "legend1 = plt.legend(*scatter.legend_elements(),\n",
        "                    loc=\"upper right\", title=\"Clusters\")\n",
        "plt.gca().add_artist(legend1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_esc_3d = PCA(n_components=3)\n",
        "reduced_data_3d = pca_esc_3d.fit_transform(df_encoded_all_esc)\n",
        "\n",
        "explained_variance_ratio = pca_esc_3d.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "print(f\"Variance explained by the first three principal components: {cumulative_variance_ratio[2]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Também criamos um gráfico em 3D:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 9))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], \n",
        "                     c=labels, cmap='viridis')\n",
        "\n",
        "ax.set_xlabel(f'First PC ({explained_variance_ratio[0]:.2%} variance)')\n",
        "ax.set_ylabel(f'Second PC ({explained_variance_ratio[1]:.2%} variance)')\n",
        "ax.set_zlabel(f'Third PC ({explained_variance_ratio[2]:.2%} variance)')\n",
        "ax.set_title('K-means Clustering (4 clusters) - 3D PCA Visualization')\n",
        "\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando os dois gráficos, é possível observar que existe uma divisão extremamente harmoniosa em cada grupo. Esse é um bom sinal, pois mostra que o nosso algoritmo K-Means foi capaz de identificar cada grupo com facilidade. Agora, vamos criar uma coluna em nossa base de dados para identificar como cada segurado está dividido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_k_means_esc_all = df_cleaned.copy()\n",
        "df_k_means_esc_all['group'] = labels\n",
        "df_k_means_esc_all.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, criaremos quatro base de dados filtrando cada grupo, isso será importante porque agora nosso trabalho é entender como o K-Means realizou a divisão de cada grupo, assim, podemos extrair insights significativos de cada grupo. Logo abaixo, iremos realizar uma análise profunda em cada grupo e realizar a discussão sobre os resultados do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise Exploratória dos Clusters do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_k_means_group_0 = df_k_means_esc_all[df_k_means_esc_all['group'] == 0]\n",
        "df_k_means_group_1 = df_k_means_esc_all[df_k_means_esc_all['group'] == 1]\n",
        "df_k_means_group_2 = df_k_means_esc_all[df_k_means_esc_all['group'] == 2]\n",
        "df_k_means_group_3 = df_k_means_esc_all[df_k_means_esc_all['group'] == 3]\n",
        "\n",
        "# Contagem de elementos em cada cluster\n",
        "print(\"Número de elementos em cada grupo:\")\n",
        "print(len(df_k_means_group_0), \n",
        "      len(df_k_means_group_1), \n",
        "      len(df_k_means_group_2), \n",
        "      len(df_k_means_group_3))\n",
        "\n",
        "# Estatísticas descritivas dos 4 clusters\n",
        "for group in range(4): \n",
        "    print(f\"\\nGroup {group} Statistics:\")\n",
        "    group_data = df_k_means_esc_all[df_k_means_esc_all['group'] == group]\n",
        "    \n",
        "\n",
        "    print(group_data.describe())\n",
        "    \n",
        "    # Frequências das variáveis categóricas\n",
        "    for col in ['faixa_etaria_sinistro', 'descricao_plano_sinistro', 'tipo_utilizacao_sinistro']:\n",
        "        print(f\"\\n{col} distribution in Group {group}:\")\n",
        "        print(group_data[col].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essa análise exploratória de todas as colunas para cada um dos clusters, construimos gráficos sobre o agrupamento dos clusters realizados pelo modelo K-means para obtermos uma melhor visualização dos agrupamentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_data = {\n",
        "    'Cluster': ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3'],\n",
        "    'Registros': [7038, 14738, 13461, 3671],\n",
        "}\n",
        "\n",
        "df_clusters = pd.DataFrame(clusters_data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_clusters['Cluster'], df_clusters['Registros'], color='lightgreen')\n",
        "plt.title('Quantidade de Registros por Cluster')\n",
        "plt.xlabel('Clusters')\n",
        "plt.ylabel('Quantidade de Registros')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste gráfico podemos visualizar a dispersão da quantidade de registros por cluster separado utilizando o modelo K-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters = [0, 1, 2, 3]\n",
        "sinistroCounts = {\n",
        "    'REDE': [],\n",
        "    'REEMBOLSO': []\n",
        "}\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "    counts = df_cluster['tipo_utilizacao_sinistro'].value_counts()    \n",
        "    sinistroCounts['REDE'].append(counts.get('REDE', 0))\n",
        "    sinistroCounts['REEMBOLSO'].append(counts.get('REEMBOLSO', 0))\n",
        "\n",
        "width = 0.4\n",
        "x = np.arange(len(clusters))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "colors = {\n",
        "    'REDE': 'lightgreen',\n",
        "    'REEMBOLSO': 'purple'\n",
        "}\n",
        "\n",
        "for tipo_sinistro, counts in sinistroCounts.items():\n",
        "    p = ax.bar(x, counts, width, label=tipo_sinistro, color=colors[tipo_sinistro])\n",
        "\n",
        "ax.set_title('Utilização do Sinistro por Cluster (REDE vs REEMBOLSO)')\n",
        "ax.set_xlabel('Clusters')\n",
        "ax.set_ylabel('Quantidade de Sinistros')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'Cluster {i}' for i in clusters])\n",
        "\n",
        "ax.legend(title='Tipo de Sinistro')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste gráfico podemos observar a quantidade do tipo de utilização do sinistro por cluster, e um ponto muito interessante que podemos observar é que os três primeiros clusters só agruparam registros de utilização do tipo REDE, enquanto o quarto cluster separou somente registros do tipo REEMBOLSO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters = [0, 1, 2, 3]\n",
        "\n",
        "valorPagoPorCluster = []\n",
        "\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "    valor_total = df_cluster['valor_pago_sinistro'].sum()\n",
        "    valorPagoPorCluster.append(valor_total)\n",
        "\n",
        "colors = ['yellow', 'skyblue', 'lightgreen', 'purple']\n",
        "\n",
        "width = 0.6\n",
        "x = np.arange(len(clusters))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "p = ax.bar(x, np.array(valorPagoPorCluster)[clusters], width, color=[colors[i] for i in clusters])\n",
        "\n",
        "ax.bar_label(p, fmt='%.2f', label_type='edge')\n",
        "\n",
        "ax.set_title('Soma do Valor Pago por Cluster')\n",
        "ax.set_xlabel('Clusters')\n",
        "ax.set_ylabel('Valor Pago (R$)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'Cluster {i}' for i in clusters])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste gráfico podemos observar a soma do valor pago para cada cluster, com isso podemos observar que o Cluster 3 possui majoritariamente valores mais custosos para a empresa, ao compararmos o valor com a quantidade de registros por cluster, desa forma podemos concluir que os serviços do tipo de utilização REEMBOLSO é mais custoso para a empresa.\n",
        "\n",
        "Após construirmos estes gráficos sobre as informações gerais pdos clusters, construimos um gráfico para cada um dos clusters a fim de identificarmos visualmente a disperção da faixa etaria em cada um dos clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contagemFaixaEtaria = []\n",
        "\n",
        "df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == 0]\n",
        "\n",
        "faixas_etarias_presentes = sorted(df_cluster['faixa_etaria_sinistro'].unique())\n",
        "\n",
        "for faixa in faixas_etarias_presentes:\n",
        "    contagem = df_cluster[df_cluster['faixa_etaria_sinistro'] == faixa].shape[0]\n",
        "    contagemFaixaEtaria.append(contagem)\n",
        "\n",
        "width = 0.6\n",
        "x = np.arange(len(contagemFaixaEtaria))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 7))\n",
        "\n",
        "p = ax.bar(x, np.array(contagemFaixaEtaria), width, color='yellow')\n",
        "\n",
        "ax.bar_label(p, fmt='%d', label_type='edge')\n",
        "\n",
        "ax.set_title('Contagem de Sinistros por Faixa Etária no Cluster 0')\n",
        "ax.set_xlabel('Faixa Etária (Anos)')\n",
        "ax.set_ylabel('Contagem de Sinistros')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{faixa}' for faixa in faixas_etarias_presentes])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos observar que o cluster 0 agrupou somente sinistros acionados por colaboradores mais velhos das faixas 54 a 58 anos e 59 ou mais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contagemFaixaEtaria = []\n",
        "\n",
        "df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == 1]\n",
        "\n",
        "faixas_etarias_presentes = sorted(df_cluster['faixa_etaria_sinistro'].unique())\n",
        "\n",
        "for faixa in faixas_etarias_presentes:\n",
        "    contagem = df_cluster[df_cluster['faixa_etaria_sinistro'] == faixa].shape[0]\n",
        "    contagemFaixaEtaria.append(contagem)\n",
        "\n",
        "width = 0.6\n",
        "x = np.arange(len(contagemFaixaEtaria))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "p = ax.bar(x, np.array(contagemFaixaEtaria), width, color='skyblue')\n",
        "\n",
        "ax.bar_label(p, fmt='%d', label_type='edge')\n",
        "\n",
        "ax.set_title('Contagem de Sinistros por Faixa Etária no Cluster 1')\n",
        "ax.set_xlabel('Faixa Etária (Anos)')\n",
        "ax.set_ylabel('Contagem de Sinistros')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{faixa}' for faixa in faixas_etarias_presentes])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neste gráfico podemos observar que o Cluster 1 agrupou faixas etárias mais dispersas abrangendo todas as faixas etárias, porém em sua maoria adultos de 34 a 38 anos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contagemFaixaEtaria = []\n",
        "\n",
        "df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == 2]\n",
        "\n",
        "faixas_etarias_presentes = sorted(df_cluster['faixa_etaria_sinistro'].unique())\n",
        "\n",
        "for faixa in faixas_etarias_presentes:\n",
        "    contagem = df_cluster[df_cluster['faixa_etaria_sinistro'] == faixa].shape[0]\n",
        "    contagemFaixaEtaria.append(contagem)\n",
        "\n",
        "width = 0.6\n",
        "x = np.arange(len(contagemFaixaEtaria))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "p = ax.bar(x, np.array(contagemFaixaEtaria), width, color='lightgreen')\n",
        "\n",
        "ax.bar_label(p, fmt='%d', label_type='edge')\n",
        "\n",
        "ax.set_title('Contagem de Sinistros por Faixa Etária no Cluster 2')\n",
        "ax.set_xlabel('Faixa Etária (Anos)')\n",
        "ax.set_ylabel('Contagem de Sinistros')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{faixa}' for faixa in faixas_etarias_presentes])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assim como o Cluster anterior o cluster 2 agrupou faixas etárias diversificadas, porém na maioria dos registros temos adultos de 34 a 38 anos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contagemFaixaEtaria = []\n",
        "\n",
        "df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == 3]\n",
        "\n",
        "faixas_etarias_presentes = sorted(df_cluster['faixa_etaria_sinistro'].unique())\n",
        "\n",
        "for faixa in faixas_etarias_presentes:\n",
        "    contagem = df_cluster[df_cluster['faixa_etaria_sinistro'] == faixa].shape[0]\n",
        "    contagemFaixaEtaria.append(contagem)\n",
        "\n",
        "width = 0.6\n",
        "x = np.arange(len(contagemFaixaEtaria))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "p = ax.bar(x, np.array(contagemFaixaEtaria), width, color='purple')\n",
        "\n",
        "ax.bar_label(p, fmt='%d', label_type='edge')\n",
        "\n",
        "ax.set_title('Contagem de Sinistros por Faixa Etária no Cluster 3')\n",
        "ax.set_xlabel('Faixa Etária (Anos)')\n",
        "ax.set_ylabel('Contagem de Sinistros')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'{faixa}' for faixa in faixas_etarias_presentes])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assim como os clusters anteriores podemos observar neste gráfico que o Cluster 3 agrupou faixas etárias diversas, porém em sua maioria colaboradores de 34 a 38 anos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusão do resultado da análise exploratória dos Clusters\n",
        "\n",
        "Após realizarmos a análise exploratória e a construção dos gáficos anteriores dos Clusters conseguimos desenvolver algumas conclusões valiosas sobre a divisão dos 4 clusters.\n",
        "\n",
        "Cluster 0: O Cluster 0 agrupa majoritariamente segurados de 59 anos ou mais com sinistros de menor valor, predominantemente na rede credenciada (REDE). Este grupo se beneficia de programas de saúde focados na prevenção e no controle de doenças crônicas, como acompanhamento médico regular e incentivo a atividades físicas, reduzindo o risco de complicações futuras e otimizando os custos dos planos de saúde.\n",
        "\n",
        "Cluster 1: O Cluster 1 agrupa em maioria segurados de 34 a 38 anos, com sinistros de custo médio, também utilizando a REDE. Este grupo demanda programas de saúde ocupacional e de bem-estar, que incentivem a prevenção de doenças e o monitoramento da saúde mental e física, promovendo a produtividade e reduzindo os custos com sinistros.\n",
        "\n",
        "Cluster 2: O Cluster 2, semelhante ao Cluster 1, apresenta sinistros de menor valor e maioria de segurados na faixa etária de 34 a 38 anos. Programas de prevenção e check-ups regulares seriam importates para manter os custos baixos e evitar o desenvolvimento de condições crônicas, além de promover hábitos saudáveis por meio de incentivos à prática de atividades físicas e uma boa nutrição.\n",
        "\n",
        "Cluster 3: O Cluster 3 agrupa segurados que utilizam reembolso (REEMBOLSO), resultando em sinistros de alto valor. Para este grupo, seria importante implementar programas de gestão de custos, incentivando o uso de sinistros de REDE e educando os segurados sobre a importância de evitar procedimentos caros fora da rede."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Novos Modelos de Clustering e Métricas de Avaliação\n",
        "\n",
        "Nesta etapa do projeto, vamos expandir nossa análise de clustering testando três novos modelos além do K-means: DBSCAN, Gaussian Mixture Models (GMM) e HDBSCAN. Cada um desses modelos oferece uma abordagem única para a tarefa de clustering, permitindo-nos explorar diferentes perspectivas sobre nossos dados.\n",
        "\n",
        "## Novos Modelos de Clustering\n",
        "\n",
        "### 1. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
        "O DBSCAN é um algoritmo baseado em densidade que agrupa pontos que estão próximos uns dos outros no espaço de dados. Ele é particularmente eficaz na identificação de clusters de formato arbitrário e na detecção de outliers.\n",
        "\n",
        "### 2. Gaussian Mixture Models (GMM)\n",
        "GMM é um modelo probabilístico que assume que os dados são gerados a partir de uma mistura de um número finito de distribuições gaussianas com parâmetros desconhecidos. Este modelo é flexível e pode capturar clusters com formas elípticas.\n",
        "\n",
        "### 3. HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)\n",
        "HDBSCAN é uma extensão do DBSCAN que cria uma hierarquia de clusters baseados em densidade. Ele oferece maior flexibilidade em comparação ao DBSCAN, sendo capaz de identificar clusters de densidades variadas.\n",
        "\n",
        "## Importância de Testar Vários Modelos\n",
        "\n",
        "Testar múltiplos modelos de clustering é vital por várias razões:\n",
        "\n",
        "1. Diferentes conjuntos de dados têm estruturas diferentes, e nem todos os algoritmos são igualmente eficazes para todos os tipos de dados.\n",
        "2. Cada modelo tem suas próprias suposições e vieses, e comparar vários modelos nos ajuda a entender melhor a estrutura subjacente dos dados.\n",
        "3. A comparação de modelos pode revelar insights que não seriam aparentes ao usar apenas um único algoritmo.\n",
        "4. Isso nos permite escolher o modelo mais apropriado para nosso conjunto de dados específico.\n",
        "\n",
        "## Métricas de Avaliação\n",
        "\n",
        "Para comparar o desempenho dos diferentes modelos, utilizaremos três métricas de avaliação:\n",
        "\n",
        "### 1. Silhouette Score\n",
        "- **Funcionamento**: Mede o quão similar um objeto é ao seu próprio cluster em comparação com outros clusters.\n",
        "- **Interpretação**: Varia de -1 a 1. Valores próximos a 1 indicam que o objeto está bem correspondido ao seu próprio cluster e mal correspondido aos clusters vizinhos. Valores próximos a 0 indicam que o objeto está na fronteira entre dois clusters. Valores negativos sugerem que o objeto pode ter sido atribuído ao cluster errado.\n",
        "\n",
        "### 2. Calinski-Harabasz Index\n",
        "- **Funcionamento**: Também conhecido como Critério de Razão de Variância, este índice é definido como a razão entre a dispersão entre clusters e a dispersão dentro dos clusters.\n",
        "- **Interpretação**: Quanto maior o valor do índice, melhor é a definição dos clusters. Um valor mais alto indica que os clusters são densos e bem separados.\n",
        "\n",
        "### 3. Davies-Bouldin Index\n",
        "- **Funcionamento**: Calcula a similaridade média entre cada cluster e seu cluster mais similar, onde a similaridade é a razão da distância dentro do cluster pela distância entre clusters.\n",
        "- **Interpretação**: Valores mais baixos indicam melhor clustering. Um valor baixo indica que os clusters são compactos e bem separados uns dos outros.\n",
        "\n",
        "Ao utilizar estas métricas em conjunto, poderemos obter uma visão abrangente da qualidade dos clusters produzidos por cada modelo, permitindo-nos fazer uma comparação objetiva e informada entre eles.\n",
        "\n",
        "## Hyperparameter Tuning\n",
        "\n",
        "### O que é Hyperparameter Tuning?\n",
        "Hyperparameter Tuning é o processo de otimização dos parâmetros de um modelo de machine learning que não são aprendidos diretamente dos dados durante o treinamento. Esses parâmetros, chamados de hiperparâmetros, controlam o comportamento do algoritmo e podem ter um impacto significativo no desempenho do modelo.\n",
        "\n",
        "### Importância do Hyperparameter Tuning\n",
        "1. **Melhoria de Desempenho**: Ajustar os hiperparâmetros pode levar a melhorias significativas no desempenho do modelo.\n",
        "2. **Adaptação ao Problema**: Permite adaptar o modelo às características específicas do conjunto de dados e do problema em questão.\n",
        "3. **Evitar Overfitting/Underfitting**: Ajuda a encontrar o equilíbrio certo entre a complexidade do modelo e sua capacidade de generalização.\n",
        "4. **Otimização de Recursos**: Pode ajudar a otimizar o uso de recursos computacionais, encontrando configurações eficientes.\n",
        "\n",
        "### ParameterGrid para Hyperparameter Tuning\n",
        "\n",
        "Para realizar o Hyperparameter Tuning, utilizaremos o ParameterGrid da biblioteca scikit-learn.\n",
        "\n",
        "#### O que é ParameterGrid?\n",
        "ParameterGrid é uma ferramenta que gera todas as combinações de parâmetros especificados em um dicionário. Ele é particularmente útil para a busca exaustiva de hiperparâmetros, também conhecida como Grid Search.\n",
        "\n",
        "#### Como funciona o ParameterGrid:\n",
        "1. **Definição**: Você define um dicionário onde as chaves são os nomes dos hiperparâmetros e os valores são listas ou arrays de valores possíveis para cada hiperparâmetro.\n",
        "\n",
        "2. **Geração de Combinações**: O ParameterGrid gera todas as combinações possíveis dos valores especificados para cada hiperparâmetro.\n",
        "\n",
        "3. **Iteração**: Você pode iterar sobre essas combinações, testando cada uma delas no seu modelo.\n",
        "\n",
        "4. **Avaliação**: Para cada combinação, o modelo é treinado e avaliado, permitindo identificar a combinação que oferece o melhor desempenho.\n",
        "\n",
        "Ao utilizar o ParameterGrid para Hyperparameter Tuning, seremos capazes de explorar sistematicamente diferentes configurações para cada um dos nossos modelos de clustering, garantindo que encontremos as melhores configurações possíveis para o nosso conjunto de dados específico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primeiro, antes de começar a criar novos modelos. Vamos criar uma função extremamente importante que receberá a base de dados e o agrupamento realizado pelo modelo nessa base de dados. A função `evaluate_clustering` retornará as três métricas que vamos trabalhar e que foram explicadas acima: `silhouette, calinski, davies`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_clustering(X, labels):\n",
        "    if len(np.unique(labels)) < 2:\n",
        "        return -np.inf, -np.inf, np.inf\n",
        "    \n",
        "    silhouette = silhouette_score(X, labels)\n",
        "    calinski = calinski_harabasz_score(X, labels)\n",
        "    davies = davies_bouldin_score(X, labels)\n",
        "    return silhouette, calinski, davies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, começaremos a treinar novos modelos para no final, comparar as métricas entre geradas e identificar os melhores modelos para nosso projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN \n",
        "\n",
        "Aqui criaremos uma função que receberá os a base de dados para o DBSCAN treinar, junto com seus hiperparâmetros. Essa função é importante porque assim poderemos treinar vários modelos, testar os hiperparâmetros com facilidade e decidir quais são os melhores para nosso caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_dbscan(X, eps, min_samples):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(X)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui criamos nosso a lógica para realizar o Hyperparameter Tuning e encontrar os melhores hiperparâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid_dbscan = {\n",
        "    'eps': np.arange(0.1, 2.1, 0.1),\n",
        "    'min_samples': range(2, 11)\n",
        "}\n",
        "\n",
        "best_score_dbscan = -np.inf\n",
        "best_params_dbscan = None\n",
        "results_dbscan = []\n",
        "\n",
        "for params in ParameterGrid(param_grid_dbscan):\n",
        "    labels = apply_dbscan(df_encoded_all_esc, params['eps'], params['min_samples'])\n",
        "    silhouette, calinski, davies = evaluate_clustering(df_encoded_all_esc, labels)\n",
        "    \n",
        "    results_dbscan.append({\n",
        "        'eps': params['eps'],\n",
        "        'min_samples': params['min_samples'],\n",
        "        'silhouette': silhouette,\n",
        "        'calinski': calinski,\n",
        "        'davies': davies\n",
        "    })\n",
        "    \n",
        "    if silhouette > best_score_dbscan:\n",
        "        best_score_dbscan = silhouette\n",
        "        best_params_dbscan = params\n",
        "    \n",
        "    print(f\"eps: {params['eps']}, min_samples: {params['min_samples']}, silhouette: {silhouette:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters:\")\n",
        "print(best_params_dbscan)\n",
        "print(f\"Best silhouette score: {best_score_dbscan:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o código acima demora uma eternidade para executar, aqui está o que estamos buscando:\n",
        "\n",
        "**Resultados do modelo DBSCAN**\n",
        "\n",
        "| Output    | Valor                |\n",
        "|--------------|----------------------|\n",
        "| eps          | 2.0                  |\n",
        "| min_samples  | 7                    |\n",
        "| silhouette   | 0.19251426832951582  |\n",
        "| calinski     | 912.7302215995092    |\n",
        "| davies       | 1.4955838623050175   |\n",
        "\n",
        "Esta tabela apresenta os resultados obtidos com o modelo DBSCAN, incluindo os hiperparâmetros utilizados (eps e min_samples) e as métricas de avaliação (silhouette score, índice Calinski-Harabasz e índice Davies-Bouldin)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, vamos criar um agrupamento utilzando o DBSCAN com os melhores hiperparâmetros encontrados. Depois vamos análisar em gráfico 2D e 3D como ficou o agrupamento e mais tarde vamos análisar as métricas para decidir o melhor entre todos nossos modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbscan_with_best_labels = apply_dbscan(df_encoded_all_esc, min_samples=7, eps=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=dbscan_with_best_labels, cmap='viridis')\n",
        "plt.title(f\"DBSCAN Clustering 2D (eps={best_params_dbscan['eps']}, min_samples={best_params_dbscan['min_samples']})\")\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], c=dbscan_with_best_labels, cmap='viridis')\n",
        "ax.set_title(f\"DBSCAN Clustering 3D (eps={best_params_dbscan['eps']}, min_samples={best_params_dbscan['min_samples']})\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, criamos várias visualizações para analisar os resultados do algoritmo DBSCAN com diferentes combinações de parâmetros. O código gera quatro gráficos distintos, cada um oferecendo insights sobre como os parâmetros do DBSCAN afetam as métricas de avaliação do clustering.\n",
        "\n",
        "Os primeiros três gráficos são plotados lado a lado e mostram como as três métricas de avaliação (Silhouette Score, Calinski-Harabasz Score e Davies-Bouldin Score) variam em função do parâmetro `eps` para diferentes valores de `min_samples`:\n",
        "\n",
        "1. **Silhouette Score vs. Parameters**: \n",
        "   - Mostra como o Silhouette Score varia com `eps` para cada valor de `min_samples`.\n",
        "   - Um Silhouette Score mais alto indica melhor definição dos clusters.\n",
        "\n",
        "2. **Calinski-Harabasz Score vs. Parameters**: \n",
        "   - Ilustra a variação do índice Calinski-Harabasz em relação a `eps` para cada `min_samples`.\n",
        "   - Valores mais altos do índice Calinski-Harabasz sugerem clusters mais bem definidos.\n",
        "\n",
        "3. **Davies-Bouldin Score vs. Parameters**: \n",
        "   - Apresenta como o índice Davies-Bouldin muda com `eps` para diferentes `min_samples`.\n",
        "   - Um índice Davies-Bouldin mais baixo indica uma melhor separação entre clusters.\n",
        "\n",
        "Cada linha nesses gráficos representa um valor específico de `min_samples`, permitindo comparar facilmente como diferentes combinações de parâmetros afetam o desempenho do clustering.\n",
        "\n",
        "4. O quarto gráfico é um gráfico de dispersão que oferece uma visão mais holística da relação entre os parâmetros do DBSCAN e o Silhouette Score:\n",
        "\n",
        "- O eixo x representa o valor de `eps`.\n",
        "- O eixo y representa o valor de `min_samples`.\n",
        "- Cada ponto no gráfico é colorido de acordo com o Silhouette Score correspondente.\n",
        "- Uma barra de cores ao lado do gráfico indica a escala do Silhouette Score.\n",
        "\n",
        "Este gráfico permite visualizar rapidamente quais combinações de `eps` e `min_samples` resultam em melhores Silhouette Scores, indicados por cores mais quentes na escala de cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results_dbscan)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Silhouette Score\n",
        "plt.subplot(131)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['eps'], data['silhouette'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Silhouette Score vs. Parameters')\n",
        "\n",
        "# Calinski-Harabasz Score\n",
        "plt.subplot(132)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['eps'], data['calinski'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Calinski-Harabasz Score vs. Parameters')\n",
        "\n",
        "# Davies-Bouldin Score\n",
        "plt.subplot(133)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['eps'], data['davies'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Davies-Bouldin Score vs. Parameters')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(results_df['eps'], results_df['min_samples'], \n",
        "                      c=results_df['silhouette'], cmap='viridis', \n",
        "                      s=50, alpha=0.6)\n",
        "plt.colorbar(scatter, label='Silhouette Score')\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('min_samples')\n",
        "plt.title('DBSCAN Parameters vs. Silhouette Score')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depois de criar nossos novos modelos, vamos comparar todas as métricas! Agora, vamos repetir todo o processo que fizemos acima com nosso segundo modelo, o Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian Mixture Models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_gaussian(X, n_components, covariance_type):\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)\n",
        "    labels = gmm.fit_predict(X)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid_gaussian = {\n",
        "    'n_components': range(2, 11),\n",
        "    'covariance_type': ['full', 'tied', 'diag', 'spherical']\n",
        "}\n",
        "\n",
        "best_score_gaussian = -np.inf\n",
        "best_params_gaussian = None\n",
        "results_gaussian = []\n",
        "\n",
        "for params in ParameterGrid(param_grid_gaussian):\n",
        "    labels = apply_gaussian(df_encoded_all_esc, params['n_components'], params['covariance_type'])\n",
        "    silhouette, calinski, davies = evaluate_clustering(df_encoded_all_esc, labels)\n",
        "    \n",
        "    results_gaussian.append({\n",
        "        'n_components': params['n_components'],\n",
        "        'covariance_type': params['covariance_type'],\n",
        "        'silhouette': silhouette,\n",
        "        'calinski': calinski,\n",
        "        'davies': davies\n",
        "    })\n",
        "    \n",
        "    if silhouette > best_score_gaussian:\n",
        "        best_score_gaussian = silhouette\n",
        "        best_params_gaussian = params\n",
        "    \n",
        "    print(f\"n_components: {params['n_components']}, covariance_type: {params['covariance_type']}, silhouette: {silhouette:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters:\")\n",
        "print(best_params_gaussian)\n",
        "print(f\"Best silhouette score: {best_score_gaussian:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o código acima demora uma eternidade para executar, aqui está o que estamos buscando:\n",
        "\n",
        "**Resultados do modelo Gaussian Mixture**\n",
        "\n",
        "| Output         | Valor                |\n",
        "|----------------|----------------------|\n",
        "| n_components   | 4                    |\n",
        "| covariance_type| full                 |\n",
        "| silhouette     | 0.2253971313441317   |\n",
        "| calinski       | 8185.180301875231    |\n",
        "| davies         | 1.7107242518807995   |\n",
        "\n",
        "Esta tabela apresenta os resultados obtidos com o modelo Gaussian Mixture, incluindo os hiperparâmetros utilizados (n_components e covariance_type) e as métricas de avaliação (silhouette score, índice Calinski-Harabasz e índice Davies-Bouldin)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gaussian_with_best_labels = apply_gaussian(df_encoded_all_esc, n_components=4, covariance_type='full')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=gaussian_with_best_labels, cmap='viridis')\n",
        "plt.title(f\"GMM Clustering 2D (n_components={best_params_gaussian['n_components']}, covariance_type={best_params_gaussian['covariance_type']})\")\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], c=gaussian_with_best_labels, cmap='viridis')\n",
        "ax.set_title(f\"GMM Clustering 3D (n_components={best_params_gaussian['n_components']}, covariance_type={best_params_gaussian['covariance_type']})\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results_gaussian)\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "for cov_type in results_df['covariance_type'].unique():\n",
        "    data = results_df[results_df['covariance_type'] == cov_type]\n",
        "    plt.plot(data['n_components'], data['silhouette'], label=cov_type, marker='o')\n",
        "plt.xlabel('n_components')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.legend()\n",
        "plt.title('Silhouette Score vs. Parameters')\n",
        "\n",
        "plt.subplot(132)\n",
        "for cov_type in results_df['covariance_type'].unique():\n",
        "    data = results_df[results_df['covariance_type'] == cov_type]\n",
        "    plt.plot(data['n_components'], data['calinski'], label=cov_type, marker='o')\n",
        "plt.xlabel('n_components')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.legend()\n",
        "plt.title('Calinski-Harabasz Score vs. Parameters')\n",
        "\n",
        "plt.subplot(133)\n",
        "for cov_type in results_df['covariance_type'].unique():\n",
        "    data = results_df[results_df['covariance_type'] == cov_type]\n",
        "    plt.plot(data['n_components'], data['davies'], label=cov_type, marker='o')\n",
        "plt.xlabel('n_components')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.legend()\n",
        "plt.title('Davies-Bouldin Score vs. Parameters')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HDBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_hdbscan(X, min_cluster_size, min_samples):\n",
        "    hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "    labels = hdbscan.fit_predict(X)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid_hdbscan = {\n",
        "    'min_cluster_size': range(5, 51, 5),\n",
        "    'min_samples': range(1, 11)\n",
        "}\n",
        "\n",
        "best_score_hdbscan = -np.inf\n",
        "best_params_hdbscan = None\n",
        "results_hdbscan = []\n",
        "\n",
        "for params in ParameterGrid(param_grid_hdbscan):\n",
        "    labels = apply_hdbscan(df_encoded_all_esc, params['min_cluster_size'], params['min_samples'])\n",
        "    silhouette, calinski, davies = evaluate_clustering(df_encoded_all_esc, labels)\n",
        "    \n",
        "    results_hdbscan.append({\n",
        "        'min_cluster_size': params['min_cluster_size'],\n",
        "        'min_samples': params['min_samples'],\n",
        "        'silhouette': silhouette,\n",
        "        'calinski': calinski,\n",
        "        'davies': davies\n",
        "    })\n",
        "    \n",
        "    if silhouette > best_score_hdbscan:\n",
        "        best_score_hdbscan = silhouette\n",
        "        best_params_hdbscan = params\n",
        "    \n",
        "    print(f\"min_cluster_size: {params['min_cluster_size']}, min_samples: {params['min_samples']}, silhouette: {silhouette:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters:\")\n",
        "print(best_params_hdbscan)\n",
        "print(f\"Best silhouette score: {best_score_hdbscan:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o código acima demora uma eternidade para executar, aqui está o que estamos buscando:\n",
        "\n",
        "**Resultados do modelo HDBSCAN**\n",
        "\n",
        "| Output           | Valor                |\n",
        "|------------------|----------------------|\n",
        "| min_cluster_size | 5                    |\n",
        "| min_samples      | 1                    |\n",
        "| silhouette       | 0.19251943299001167  |\n",
        "| calinski         | 39.60968216846142    |\n",
        "| davies           | 1.2197120808134985   |\n",
        "\n",
        "Esta tabela apresenta os resultados obtidos com o modelo HDBSCAN, incluindo os hiperparâmetros utilizados (min_cluster_size e min_samples) e as métricas de avaliação (silhouette score, índice Calinski-Harabasz e índice Davies-Bouldin)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hdbscan_with_best_labels = apply_hdbscan(df_encoded_all_esc,min_cluster_size=5, min_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=hdbscan_with_best_labels, cmap='viridis')\n",
        "plt.title(f\"HDBSCAN Clustering 2D (min_cluster_size={best_params_hdbscan['min_cluster_size']}, min_samples={best_params_hdbscan['min_samples']})\")\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 3D\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], c=hdbscan_with_best_labels, cmap='viridis')\n",
        "ax.set_title(f\"HDBSCAN Clustering 3D (min_cluster_size={best_params_hdbscan['min_cluster_size']}, min_samples={best_params_hdbscan['min_samples']})\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results_hdbscan)\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "plt.subplot(221)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['min_cluster_size'], data['silhouette'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('min_cluster_size')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Silhouette Score vs. Parameters')\n",
        "\n",
        "plt.subplot(222)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['min_cluster_size'], data['calinski'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('min_cluster_size')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Calinski-Harabasz Score vs. Parameters')\n",
        "\n",
        "plt.subplot(223)\n",
        "for min_samples in results_df['min_samples'].unique():\n",
        "    data = results_df[results_df['min_samples'] == min_samples]\n",
        "    plt.plot(data['min_cluster_size'], data['davies'], label=f'min_samples={min_samples}', marker='o')\n",
        "plt.xlabel('min_cluster_size')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.legend(title='min_samples', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.title('Davies-Bouldin Score vs. Parameters')\n",
        "\n",
        "plt.subplot(224)\n",
        "scatter = plt.scatter(results_df['min_cluster_size'], results_df['min_samples'], \n",
        "                      c=results_df['silhouette'], cmap='viridis', \n",
        "                      s=50, alpha=0.6)\n",
        "plt.colorbar(scatter, label='Silhouette Score')\n",
        "plt.xlabel('min_cluster_size')\n",
        "plt.ylabel('min_samples')\n",
        "plt.title('HDBSCAN Parameters vs. Silhouette Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-means\n",
        "\n",
        "Para o K-means, também fazer realizar a busca pelos melhores hiperparâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_distributions = {\n",
        "    'n_clusters': range(2, 21),\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'n_init': [10, 20, 30]\n",
        "}\n",
        "\n",
        "best_score_kmeans = -np.inf\n",
        "best_params_kmeans = None\n",
        "results_kmeans = []\n",
        "\n",
        "n_iter = 100\n",
        "\n",
        "# Gerando combinações aleatórias de parâmetros\n",
        "param_list = list(ParameterSampler(param_distributions, n_iter=n_iter, random_state=42))\n",
        "\n",
        "for params in param_list:\n",
        "    kmeans = KMeans(**params, random_state=42)\n",
        "    labels = kmeans.fit_predict(df_encoded_all_esc)\n",
        "    silhouette, calinski, davies = evaluate_clustering(df_encoded_all_esc, labels)\n",
        "    \n",
        "    results_kmeans.append({\n",
        "        'n_clusters': params['n_clusters'],\n",
        "        'init': params['init'],\n",
        "        'max_iter': params['max_iter'],\n",
        "        'n_init': params['n_init'],\n",
        "        'silhouette': silhouette,\n",
        "        'calinski': calinski,\n",
        "        'davies': davies\n",
        "    })\n",
        "    \n",
        "    if silhouette > best_score_kmeans:\n",
        "        best_score_kmeans = silhouette\n",
        "        best_params_kmeans = params\n",
        "    \n",
        "    print(f\"n_clusters: {params['n_clusters']}, init: {params['init']}, \"\n",
        "          f\"max_iter: {params['max_iter']}, n_init: {params['n_init']}, \"\n",
        "          f\"silhouette: {silhouette:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters:\")\n",
        "print(best_params_kmeans)\n",
        "print(f\"Best silhouette score: {best_score_kmeans:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como o código acima demora uma eternidade para executar, aqui está o que estamos buscando:\n",
        "\n",
        "**Resultados do modelo K-means**\n",
        "\n",
        "| Output     | Valor                |\n",
        "|------------|----------------------|\n",
        "| n_clusters | 2                    |\n",
        "| init       | k-means++            |\n",
        "| max_iter   | 300                  |\n",
        "| n_init     | 30                   |\n",
        "| silhouette | 0.3772063033450325   |\n",
        "| calinski   | 8661.77370603141     |\n",
        "| davies     | 1.204260433186076    |\n",
        "\n",
        "Esta tabela apresenta os resultados obtidos com o modelo K-means, incluindo os hiperparâmetros utilizados (n_clusters, init, max_iter e n_init) e as métricas de avaliação (silhouette score, índice Calinski-Harabasz e índice Davies-Bouldin)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans_with_best_params = KMeans(random_state=42, n_clusters=2, init='k-means++', max_iter=300, n_init=30)\n",
        "labels = kmeans_with_best_params.fit_predict(df_encoded_all_esc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2D plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='viridis')\n",
        "plt.title(\"KMeans Clustering 2D (n_clusters=2)\")\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 3D plot\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], c=labels, cmap='viridis')\n",
        "ax.set_title(\"KMeans Clustering 3D (n_clusters=2)\")\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results_kmeans)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(results_df['n_clusters'], results_df['silhouette'], marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. Number of Clusters')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(results_df['n_clusters'], results_df['calinski'], marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Calinski-Harabasz Score vs. Number of Clusters')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(results_df['n_clusters'], results_df['davies'], marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score vs. Number of Clusters')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análise dos Métodos de Seleção de Clusters para o K-means\n",
        "\n",
        "Realizamos duas abordagens principais para determinar o número ideal de clusters:\n",
        "\n",
        "1. **Hyperparameter Tuning com Silhouette Score**: Esta análise sugeriu que 2 clusters seriam ideais, baseado na maximização do score de silhueta.\n",
        "\n",
        "2. **Método do Cotovelo (Elbow Method)**: Este método indicou que 4 clusters seriam mais apropriados.\n",
        "\n",
        "### Decisão: Utilização de 4 Clusters\n",
        "\n",
        "Apesar do Silhouette Score favorecer 2 clusters, optamos por utilizar 4 clusters. Esta decisão se baseia nas seguintes considerações, alinhadas com o propósito do projeto:\n",
        "\n",
        "1. **Granularidade na Segmentação**: Com 4 clusters, podemos obter uma segmentação mais detalhada dos colaboradores, permitindo uma análise mais refinada dos diferentes perfis de saúde e comportamentos de utilização do plano de saúde.\n",
        "\n",
        "2. **Personalização de Programas de Saúde**: Um dos objetivos principais é \"criar programas de saúde que venham de encontro ao perfil e necessidade dos colaboradores\". Quatro grupos permitem uma maior customização desses programas, atendendo de forma mais precisa às necessidades específicas de cada segmento.\n",
        "\n",
        "3. **Alinhamento com a Complexidade do Problema**: Considerando a variedade de fatores que influenciam a saúde dos colaboradores (como doenças crônicas, hábitos de vida, perfil demográfico), 4 clusters oferecem uma representação mais realista dessa complexidade.\n",
        "\n",
        "4. **Potencial para Insights Mais Ricos**: Com uma segmentação mais detalhada, aumentamos a probabilidade de descobrir padrões e tendências que poderiam passar despercebidos em uma divisão mais ampla.\n",
        "\n",
        "5. **Flexibilidade na Criação de Programas**: Quatro clusters nos dão mais flexibilidade para \"fazer o que faz sentido e não somente por práticas de mercado\", como mencionado nos objetivos do projeto.\n",
        "\n",
        "6. **Suporte à Análise Preditiva**: Para \"criar modelos de previsão e tendências\", uma segmentação em 4 grupos nos permite capturar nuances mais sutis no comportamento dos colaboradores, potencialmente melhorando a precisão das previsões.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "Embora o Silhouette Score tenha favorecido 2 clusters, a decisão de usar 4 clusters está mais alinhada com os objetivos específicos deste projeto para a Unipar. Esta abordagem nos permite uma análise mais detalhada e personalizada, crucial para o desenvolvimento de programas de saúde eficazes e direcionados. Continuaremos monitorando e validando esta decisão à medida que avançamos no projeto, sempre abertos a ajustes baseados em insights adicionais ou feedback dos stakeholders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4,init='k-means++',random_state=42,max_iter=300, n_init=30)\n",
        "labels = kmeans.fit_predict(df_encoded_all_esc)\n",
        "\n",
        "silhouette, calinski, davies = evaluate_clustering(df_encoded_all_esc, labels)\n",
        "print(silhouette, calinski, davies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por causa da nossa decisão de continuar com o agrupamento em 4 clusters, as métricas passam a ser:\n",
        "\n",
        "**Resultados do modelo K-means**\n",
        "\n",
        "| Output     | Valor                |\n",
        "|------------|----------------------|\n",
        "| n_clusters | 4                    |\n",
        "| init       | k-means++            |\n",
        "| max_iter   | 300                  |\n",
        "| n_init     | 30                   |\n",
        "| silhouette | 0.22550012925911192  |\n",
        "| calinski   | 8187.454893885175    |\n",
        "| davies     | 1.7098871435121716   |\n",
        "\n",
        "Esta tabela apresenta os resultados obtidos com o modelo K-means, incluindo os hiperparâmetros utilizados (n_clusters, init, max_iter e n_init) e as métricas de avaliação (silhouette score, índice Calinski-Harabasz e índice Davies-Bouldin)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparativas dos modelos\n",
        "\n",
        "| Modelo           | Silhouette Score | Índice Calinski-Harabasz | Índice Davies-Bouldin |\n",
        "|------------------|-------------------|--------------------------|------------------------|\n",
        "| K-means          | 0.2255            | 8187.4549                | 1.7099                 |\n",
        "| HDBSCAN          | 0.1925            | 39.6097                  | 1.2197                 |\n",
        "| Gaussian Mixture | 0.2254            | 8185.1803                | 1.7107                 |\n",
        "| DBSCAN           | 0.1925            | 912.7302                 | 1.4956                 |\n",
        "\n",
        "## Análise Comparativa\n",
        "\n",
        "1. **Silhouette Score**:\n",
        "   - K-means e Gaussian Mixture têm os melhores scores (0.2255 e 0.2254 respectivamente), indicando uma melhor separação e coesão dos clusters.\n",
        "   - HDBSCAN e DBSCAN têm scores mais baixos (ambos cerca de 0.1925), sugerindo clusters menos bem definidos.\n",
        "\n",
        "2. **Índice Calinski-Harabasz**:\n",
        "   - K-means e Gaussian Mixture têm os valores mais altos (8187 e 8185 respectivamente), indicando clusters mais densos e bem separados.\n",
        "   - DBSCAN tem um valor moderado (912), enquanto HDBSCAN tem um valor muito baixo (39.6), sugerindo que estes modelos baseados em densidade podem estar identificando estruturas de cluster diferentes.\n",
        "\n",
        "3. **Índice Davies-Bouldin**:\n",
        "   - HDBSCAN tem o menor valor (1.2197), seguido por DBSCAN (1.4956), indicando uma melhor separação entre clusters para estes modelos baseados em densidade.\n",
        "   - K-means e Gaussian Mixture têm valores mais altos (1.7099 e 1.7107 respectivamente), sugerindo uma separação menos clara entre os clusters.\n",
        "\n",
        "### Conclusões:\n",
        "\n",
        "1. **K-means e Gaussian Mixture** mostram desempenho muito semelhante em todas as métricas. Eles são superiores em termos de Silhouette Score e Calinski-Harabasz, indicando clusters bem definidos e separados. No entanto, seu índice Davies-Bouldin mais alto sugere que pode haver alguma sobreposição entre os clusters.\n",
        "\n",
        "2. **HDBSCAN e DBSCAN** têm Silhouette Scores mais baixos, mas HDBSCAN se destaca com o melhor (menor) índice Davies-Bouldin. Isso sugere que, embora os clusters possam não ser tão bem separados internamente, eles são mais distintos uns dos outros.\n",
        "\n",
        "3. O **baixo índice Calinski-Harabasz do HDBSCAN** é notável e pode indicar que este modelo está identificando uma estrutura de cluster muito diferente dos outros modelos, possivelmente capturando clusters de formas não convencionais ou lidando melhor com ruído nos dados.\n",
        "\n",
        "4. **DBSCAN** parece ter um desempenho intermediário, com um bom equilíbrio entre as diferentes métricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Modelo MeanShift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O algoritmo Mean Shift é uma técnica de agrupamento (clustering) usado em aprendizado de máquina e visão computacional para identificar regiões densas em um espaço de características. Ele é frequentemente aplicado em problemas de segmentação de imagem e rastreamento de objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_meanshift = df_cleaned.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pré-processamento e Análise de Componentes Principais (PCA)\n",
        "Nesta seção, realizamos o pré-processamento dos dados e aplicamos a Análise de Componentes Principais (PCA) para reduzir a dimensionalidade do conjunto de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo num_cols e cat_cols\n",
        "num_cols = df_meanshift.select_dtypes(include=['int32', 'int64', 'float64']).columns.tolist()\n",
        "cat_cols = df_meanshift.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Pipeline para pré-processamento\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)  # Forçar saída densa\n",
        "    ]\n",
        ")\n",
        "\n",
        "# PCA com 2 componentes\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Pipeline para pré-processamento e PCA\n",
        "pca_pipeline = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('pca', pca)\n",
        "])\n",
        "\n",
        "# Aplicar o pipeline e obter os componentes principais\n",
        "X_pca = pca_pipeline.fit_transform(df_cleaned)\n",
        "\n",
        "# Criar o DataFrame para variância explicada\n",
        "df_var = pd.DataFrame({\n",
        "    'var': pca.explained_variance_ratio_,\n",
        "    'PC': ['PC1', 'PC2']\n",
        "})\n",
        "\n",
        "# Gráfico 1: PCA scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 1], cmap='viridis')  # Ajuste no parâmetro 'c' para color mapping\n",
        "\n",
        "plt.title(\"PCA - Componentes Principais\")\n",
        "plt.xlabel(\"Componente Principal 1\")\n",
        "plt.ylabel(\"Componente Principal 2\")\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "# Ajustar layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O gráfico de dispersão gerado pelo código, mostra a projeção dos dados originais em dois novos eixos (Componentes Principais 1 e 2). Esses eixos são combinações lineares das variáveis originais e foram escolhidos para capturar a maior parte da variabilidade dos dados. Cada ponto no gráfico representa uma observação ou amostra do conjunto de dados\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "--------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Detecção Automática de `Bandwidth` e Aplicação do MeanShift\n",
        "Neste trecho de código, aplicamos o algoritmo de clustering MeanShift aos dados transformados pelo PCA e visualizamos os clusters gerados, tanto em 2D quanto em 3D, além de analisar a variância explicada pelos componentes principais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Aplicar o MeanShift\n",
        "meanshift = MeanShift()\n",
        "cluster_labels = meanshift.fit_predict(X_pca)\n",
        "\n",
        "# Adicionar os rótulos de cluster ao DataFrame original\n",
        "df_meanshift['cluster'] = cluster_labels\n",
        "\n",
        "# Criando os subplots com 2 linhas e 2 colunas\n",
        "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Gráfico 1: Plot dos clusters (PCA 2D)\n",
        "axs[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.5)\n",
        "axs[0, 0].set_title('Clusters Identificados pelo MeanShift nos Componentes Principais (PCA)')\n",
        "axs[0, 0].set_xlabel('Componente Principal 1')\n",
        "axs[0, 0].set_ylabel('Componente Principal 2')\n",
        "plt.colorbar(axs[0, 0].collections[0], ax=axs[0, 0], label='Cluster Label')\n",
        "axs[0, 0].grid(True)\n",
        "\n",
        "# PCA com 3 componentes\n",
        "pca_3d = PCA(n_components=3)\n",
        "\n",
        "# Pipeline para pré-processamento e PCA (com 3 componentes)\n",
        "pca_pipeline_3d = Pipeline(steps=[\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('pca', pca_3d)\n",
        "])\n",
        "\n",
        "# Aplicar o pipeline e obter os componentes principais\n",
        "X_pca_3D = pca_pipeline_3d.fit_transform(df_meanshift)\n",
        "\n",
        "# Gráfico 2: PCA 3D plot\n",
        "ax_3d = fig.add_subplot(223, projection='3d')\n",
        "scatter_3d = ax_3d.scatter(X_pca_3D[:, 0], X_pca_3D[:, 1], X_pca_3D[:, 2],\n",
        "                        c=cluster_labels, cmap='viridis')\n",
        "ax_3d.set_title('PCA com 3 Componentes')\n",
        "ax_3d.set_xlabel('Componente Principal 1')\n",
        "ax_3d.set_ylabel('Componente Principal 2')\n",
        "ax_3d.set_zlabel('Componente Principal 3')\n",
        "fig.colorbar(scatter_3d, ax=ax_3d, label='Cluster Label')\n",
        "\n",
        "# Gráfico 3: Variância explicada pelos componentes (PCA com 2 componentes)\n",
        "df_var = pd.DataFrame({\n",
        "    'var': pca.explained_variance_ratio_,\n",
        "    'PC': ['PC1', 'PC2']\n",
        "})\n",
        "\n",
        "sns.barplot(x='PC', y=\"var\", data=df_var, color=\"c\", ax=axs[0, 1])\n",
        "axs[0, 1].set_title(\"Variância Explicada por Componente (PCA 2D)\")\n",
        "axs[0, 1].set_ylabel(\"Proporção da Variância Explicada\")\n",
        "axs[0, 1].set_xlabel(\"Componentes Principais\")\n",
        "\n",
        "# Gráfico 4: Variância explicada pelos componentes (PCA com 3 componentes)\n",
        "df_var_3D = pd.DataFrame({\n",
        "    'var': pca_3d.explained_variance_ratio_,\n",
        "    'PC': ['PC1', 'PC2', 'PC3']\n",
        "})\n",
        "\n",
        "sns.barplot(x='PC', y=\"var\", data=df_var_3D, color=\"c\", ax=axs[1, 1])\n",
        "axs[1, 1].set_title(\"Variância Explicada por Componente (PCA 3D)\")\n",
        "axs[1, 1].set_ylabel(\"Proporção da Variância Explicada\")\n",
        "axs[1, 1].set_xlabel(\"Componentes Principais\")\n",
        "\n",
        "# Ajuste do layout para evitar sobreposição\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar o gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Gráfico Superior Esquerdo: PCA com 2 Componentes (2D)\n",
        "Este gráfico 2D exibe a projeção dos dados em dois componentes principais (PC1 e PC2). Novamente, os clusters identificados pelo MeanShift são representados por diferentes cores. O gráfico permite uma visão mais rica das relações entre as amostras ao incluir um terceiro eixo.\n",
        "\n",
        "##### Gráfico Superior Direito: Variância Explicada (PCA 2D)\n",
        "Este gráfico de barras mostra a proporção da variância explicada pelos dois primeiros componentes principais. Cada componente (PC1 e PC2) explica cerca de 14% da variância dos dados. Juntos, capturam aproximadamente 28% da variância total.\n",
        "\n",
        "##### Gráfico Inferior Esquerdo: PCA com 3 Componentes (3D)\n",
        "Este gráfico 3D exibe a projeção dos dados em três componentes principais (PC1, PC2 e PC3). Novamente, os clusters identificados pelo MeanShift são representados por diferentes cores. O gráfico permite uma visão mais rica das relações entre as amostras ao incluir um terceiro eixo.\n",
        "\n",
        "##### Gráfico Inferior Direito: Variância Explicada (PCA 3D)\n",
        "Aqui, o gráfico de barras mostra a proporção da variância explicada pelos três primeiros componentes principais. O PC1 e o PC2 explicam cerca de 14%, enquanto o PC3 explica aproximadamente 10%. Isso indica que esses três componentes capturam cerca de 38% da variância total dos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análise de Clusters Identificados pelo MeanShift\n",
        "O código a seguir, divide o DataFrame `df_meanshift` em subgrupos com base na coluna `'cluster'`, gerada pelo algoritmo MeanShift. Ele conta o número de elementos em cada cluster e exibe estatísticas descritivas (média, desvio padrão, etc.) para cada um. Também mostra a distribuição de variáveis categóricas selecionadas, permitindo uma análise detalhada de cada cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_mean_shift_cluster_0 = df_meanshift[df_meanshift['cluster'] == 0]\n",
        "df_mean_shift_cluster_1 = df_meanshift[df_meanshift['cluster'] == 1]\n",
        "df_mean_shift_cluster_2 = df_meanshift[df_meanshift['cluster'] == 2]\n",
        "df_mean_shift_cluster_3 = df_meanshift[df_meanshift['cluster'] == 3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Contagem de elementos em cada cluster\n",
        "print(\"Número de elementos em cada grupo:\")\n",
        "print(len(df_mean_shift_cluster_0), \n",
        "      len(df_mean_shift_cluster_1), \n",
        "      len(df_mean_shift_cluster_2), \n",
        "      len(df_mean_shift_cluster_3))\n",
        "\n",
        "\n",
        "# Estatísticas descritivas dos 4 clusters\n",
        "for cluster in range(4): \n",
        "    print(f\"\\nCluster {cluster} Statistics:\")\n",
        "    cluster_data = df_meanshift[df_meanshift['cluster'] == cluster]\n",
        "    \n",
        "\n",
        "    print(cluster_data.describe())\n",
        "    \n",
        "    # Frequências das variáveis categóricas\n",
        "    for col in ['faixa_etaria_sinistro', 'descricao_plano_sinistro', 'tipo_utilizacao_sinistro', 'descricao_servico_sinistro']:\n",
        "        print(f\"\\n{col} distribution in cluster {cluster}:\")\n",
        "        print(cluster_data[col].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Interpretação do Output\n",
        "\n",
        "O output fornecido apresenta uma análise detalhada dos clusters formados pelo algoritmo MeanShift. Aqui estão os principais pontos que podem ser interpretados a partir dos dados:\n",
        "\n",
        "1. **Distribuição dos Elementos nos Clusters**:\n",
        "   - **Cluster 0**: 21.299 elementos\n",
        "   - **Cluster 1**: 10.385 elementos\n",
        "   - **Cluster 2**: 5.107 elementos\n",
        "   - **Cluster 3**: 2.117 elementos\n",
        "\n",
        "2. **Cluster 0**:\n",
        "   - Este cluster é o maior, com 21.299 elementos.\n",
        "   - A maioria dos segurados está na faixa etária entre 34 e 48 anos.\n",
        "   - O plano mais comum é o `TQN2`, seguido por `NP2X`.\n",
        "   - O tipo de utilização predominante é \"REDE\".\n",
        "   - Os serviços mais frequentemente utilizados incluem \"CONSULTA CONSULTORIO\" e \"SESSAO DE PSICOTERAPIA INDIVIDUAL\".\n",
        "\n",
        "3. **Cluster 1**:\n",
        "   - Com 10.385 elementos, é o segundo maior cluster.\n",
        "   - A faixa etária dominante é também entre 34 e 48 anos, mas há uma maior concentração de segurados de 34 a 38 anos.\n",
        "   - Novamente, o plano `TQN2` é o mais prevalente.\n",
        "   - A maioria dos segurados utiliza a \"REDE\" para serviços, com \"CONSULTA CONSULTORIO\" sendo o serviço mais comum.\n",
        "\n",
        "4. **Cluster 2**:\n",
        "   - Contém 5.107 elementos, com uma clara predominância de segurados com 59 anos ou mais (4.812 elementos).\n",
        "   - Os planos `TN1E` e `TNQ2` são os mais comuns.\n",
        "   - Os serviços mais comuns incluem \"CONSULTA CONSULTORIO\" e exames laboratoriais como \"CREATININA\" e \"UREIA\".\n",
        "\n",
        "5. **Cluster 3**:\n",
        "   - Este cluster tem 2.117 elementos.\n",
        "   - Como no Cluster 2, há uma alta concentração de segurados de 59 anos ou mais (1.981 elementos).\n",
        "   - Os planos mais comuns são `TN1E` e `TQN2`.\n",
        "   - Serviços médicos utilizados são semelhantes aos do Cluster 2, com uma ênfase em \"CONSULTA CONSULTORIO\".\n",
        "\n",
        "\n",
        "##### Conclusões\n",
        "\n",
        "- **Distribuição Desigual**: Os clusters são bastante desiguais em tamanho, com os dois primeiros clusters (0 e 1) contendo a maioria dos elementos, o que pode indicar uma maior homogeneidade nos dados desses grupos.\n",
        "- **Faixas Etárias**: Há uma distinção clara de faixas etárias entre os clusters, especialmente entre os segurados mais jovens (clusters 0 e 1) e os mais velhos (clusters 2 e 3).\n",
        "- **Utilização dos Serviços**: Certos serviços médicos e planos de saúde dominam dentro de cada cluster, o que pode ser útil para segmentação de mercado ou para direcionar campanhas específicas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste de Parâmetros do MeanShift com Avaliação do Silhouette Score\n",
        "Nesta seção, realizamos uma busca manual pelos melhores hiperparâmetros do algoritmo de clustering MeanShift, utilizando métricas de avaliação para determinar a qualidade dos clusters formados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_meanshift_clustering(X, bandwidth_values):\n",
        "    results = []\n",
        "    \n",
        "    for bandwidth in bandwidth_values:\n",
        "        model = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "        labels = model.fit_predict(X)\n",
        "        n_clusters = len(np.unique(labels))\n",
        "        print(f\"Bandwidth: {bandwidth}, Number of Clusters: {n_clusters}\")\n",
        "        if n_clusters > 1:\n",
        "            score = silhouette_score(X, labels)\n",
        "            results.append((bandwidth, n_clusters, score))\n",
        "        else:\n",
        "            results.append((bandwidth, n_clusters, np.nan))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Testar diferentes valores de bandwidth\n",
        "bandwidth_values = [0.1, 0.5, 1, 2, 3, 4]\n",
        "results = test_meanshift_clustering(X_pca, bandwidth_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Resultados do teste no modelo MeanShift**\n",
        "\n",
        "| Bandwidth | Number of Clusters |\n",
        "|-----------|--------------------|\n",
        "| 0.1       | 560                |\n",
        "| 0.5       | 8                  |\n",
        "| 1         | 4                  |\n",
        "| 2         | 1                  |\n",
        "| 3         | 1                  |\n",
        "| 4         |  1                 |\n",
        "\n",
        "\n",
        "O output acima mostra que o número de clusters formados pelo MeanShift varia significativamente com o valor de bandwidth.\n",
        "\n",
        " - Com bandwidth = 0.1, o algoritmo forma 560 clusters, indicando uma fragmentação excessiva dos dados.\n",
        "\n",
        " - À medida que o bandwidth aumenta, o número de clusters diminui, sugerindo uma maior coesão dos dados.\n",
        "\n",
        " - Para bandwidth ≥ 2, apenas 1 cluster é formado, indicando um excesso de suavização.\n",
        "  \n",
        "**Hipótese**: Um valor intermediário de bandwidth (entre 0.5 e 1) parece mais adequado, evitando tanto a fragmentação excessiva quanto a aglomeração dos dados em um único cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste de Clustering com MeanShift e Diferentes Valores de Bandwidth\n",
        "Nesta seção, realizamos um teste do algoritmo MeanShift utilizando diferentes valores para o parâmetro `bandwidth`. O objetivo é avaliar como a variação deste parâmetro afeta o número de clusters formados e a qualidade dos _clusters_, medida pelo _Silhouette Score_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_scorer(estimator, X):\n",
        "    labels = estimator.fit_predict(X)\n",
        "    unique_labels = np.unique(labels)\n",
        "    \n",
        "    if len(unique_labels) > 1:\n",
        "        silhouette = silhouette_score(X, labels)\n",
        "        davies_bouldin = davies_bouldin_score(X, labels)\n",
        "        calinski_harabasz = calinski_harabasz_score(X, labels)\n",
        "        return silhouette, davies_bouldin, calinski_harabasz\n",
        "    else:\n",
        "        return float('nan'), float('nan'), float('nan')  # Retorna nan para todas as métricas se houver menos de dois clusters\n",
        "\n",
        "def grid_search_meanshift(X):\n",
        "    # Define os parâmetros a serem ajustados\n",
        "    param_grid = {\n",
        "        'bandwidth': [0.5, 1],\n",
        "        'max_iter': [300, 500],\n",
        "        'min_bin_freq': [1, 2, 3],\n",
        "        'cluster_all': [True, False]\n",
        "    }\n",
        "    \n",
        "    model = MeanShift(bin_seeding=True)\n",
        "    \n",
        "    # Vamos criar listas para armazenar as métricas\n",
        "    best_silhouette = -1\n",
        "    best_davies_bouldin = float('inf')\n",
        "    best_calinski_harabasz = -1\n",
        "    best_params = None\n",
        "\n",
        "    for bandwidth in param_grid['bandwidth']:\n",
        "        for max_iter in param_grid['max_iter']:\n",
        "            for min_bin_freq in param_grid['min_bin_freq']:\n",
        "                for cluster_all in param_grid['cluster_all']:\n",
        "                    model.set_params(bandwidth=bandwidth, max_iter=max_iter, min_bin_freq=min_bin_freq, cluster_all=cluster_all)\n",
        "                    \n",
        "                    try:\n",
        "                        silhouette, davies_bouldin, calinski_harabasz = custom_scorer(model, X)\n",
        "                        \n",
        "                        # Aqui decidimos qual conjunto de parâmetros é o \"melhor\" com base em uma das métricas\n",
        "                        # ou uma combinação delas. Neste caso, priorizamos silhouette score\n",
        "                        if silhouette > best_silhouette:\n",
        "                            best_silhouette = silhouette\n",
        "                            best_davies_bouldin = davies_bouldin\n",
        "                            best_calinski_harabasz = calinski_harabasz\n",
        "                            best_params = {\n",
        "                                'bandwidth': bandwidth,\n",
        "                                'max_iter': max_iter,\n",
        "                                'min_bin_freq': min_bin_freq,\n",
        "                                'cluster_all': cluster_all\n",
        "                            }\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Erro ao calcular as métricas: {e}\")\n",
        "                        continue\n",
        "    \n",
        "    return best_params, best_silhouette, best_davies_bouldin, best_calinski_harabasz\n",
        "\n",
        "# Supondo que X seja o resultado do PCA\n",
        "best_params, best_silhouette, best_davies_bouldin, best_calinski_harabasz = grid_search_meanshift(X_pca)\n",
        "print(f\"Melhores Parâmetros (GridSearchCV): {best_params}\")\n",
        "print(f\"Melhor Silhouette Score: {best_silhouette}\")\n",
        "print(f\"Melhor Davies-Bouldin Index: {best_davies_bouldin}\")\n",
        "print(f\"Melhor Calinski-Harabasz Index: {best_calinski_harabasz}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Hipótese com Base no Output\n",
        "\n",
        "O output indica que o melhor conjunto de parâmetros encontrados para o algoritmo MeanShift inclui um `bandwidth` de 1, `max_iter` de 300, `min_bin_freq` de 1 e `cluster_all` definido como `True`. Este modelo obteve as seguintes métricas:\n",
        "\n",
        "| Output | Valores |\n",
        "|-----------|--------------------|\n",
        "| **Silhouette Score**       | 0.6307                |\n",
        "| **Davies-Bouldin Index**       | 0.4367                  |\n",
        "| **Calinski-Harabasz Index**         | 64557.34                  |\n",
        "\n",
        "#### Conclusão: MeanShift como Modelo Preditivo\n",
        "\n",
        "Embora o MeanShift tenha mostrado resultados decentes com o melhor conjunto de parâmetros (com métricas de qualidade razoáveis), a comparação com outros modelos (como K-means, com um Silhouette Score de 0.2255, Calinski-Harabasz Index de 8187.45, e Davies-Bouldin Index de 1.7099) sugere que o MeanShift pode não ser a melhor opção para o projeto. \n",
        "\n",
        "**Razões para considerar o MeanShift inadequado**:\n",
        "\n",
        "1. **Sensibilidade ao Bandwidth**: MeanShift é altamente sensível ao parâmetro `bandwidth`, e encontrar o valor ideal pode ser difícil e dependente do domínio. Um valor inadequado pode resultar em clusters mal definidos ou excesso de suavização (formação de um único cluster).\n",
        "\n",
        "2. **Escalabilidade**: MeanShift não escala bem com grandes conjuntos de dados. Se o projeto envolve grandes volumes de dados, a eficiência do algoritmo pode se tornar um problema.\n",
        "\n",
        "3. **Clusters Esparsos e Pequenos**: Em alguns casos, MeanShift pode gerar clusters muito pequenos ou muito dispersos, o que pode ser inadequado dependendo da aplicação.\n",
        "\n",
        "4. **Comparação com Outros Modelos**: Quando comparado com outras abordagens, o modelo testado com o MeanShift apresentou métricas (Silhouette, Calinski-Harabasz, Davies-Bouldin) que não foram significativamente superiores, indicando que o esforço para ajustar o MeanShift pode não compensar.\n",
        "\n",
        "**Conclusão Geral**: Embora o MeanShift tenha sido capaz de formar clusters razoáveis, a complexidade de ajuste dos hiperparâmetros e as limitações em termos de escalabilidade e flexibilidade sugerem que ele pode não ser a melhor opção para o projeto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introdução do Modelo Final (K-Means)\n",
        "\n",
        "Após uma análise detalhada do algoritmo MeanShift, escolhemos utilizar o K-Means como modelo final devido ao seu desempenho superior em comparação com os demais modelos testados. O K-Means mostrou-se mais eficaz na segmentação dos dados e apresentou resultados mais consistentes para os objetivos da análise. Além disso, para aprofundar a compreensão e facilitar a interpretação dos clusters gerados, diversos novos gráficos foram criados, posibilitando uma melhor visualização detalhada dos dados. Esses gráficos permitem observar com maior clareza a distribuição dos clusters, bem como suas principais características e relações, contribuindo para uma análise mais completa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters = sorted(df_k_means_esc_all['group'].unique())\n",
        "homens = []\n",
        "mulheres = []\n",
        "totais = []\n",
        "\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "\n",
        "    homens_count = df_cluster[df_cluster['sexo_sinistro'] == 'M'].shape[0]\n",
        "    mulheres_count = df_cluster[df_cluster['sexo_sinistro'] == 'F'].shape[0]\n",
        "\n",
        "    homens.append(homens_count)\n",
        "    mulheres.append(mulheres_count)\n",
        "    totais.append(homens_count + mulheres_count)\n",
        "\n",
        "# Definir a figura com subplots para quatro clusters\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "# Criar lista de cores: verde escuro para homens, cinza para mulheres\n",
        "cores = ['#3abd3a', '#d4d6d5']  # Verde escuro e cinza\n",
        "\n",
        "# Para cada cluster, criar um gráfico de pizza\n",
        "for i, cluster in enumerate(clusters):\n",
        "    ax = axs[i // 2, i % 2]  # Organizando em 2x2 subplots\n",
        "    labels = ['Homens', 'Mulheres']\n",
        "    sizes = [homens[i], mulheres[i]]\n",
        "    \n",
        "    # Adicionando o tamanho das labels\n",
        "    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=cores, \n",
        "           textprops={'fontsize': 14})  # Tamanho das labels\n",
        "    \n",
        "    ax.set_title(f'Distribuição de Sexo no Cluster {cluster}', fontsize=16)  # Título maior\n",
        "\n",
        "# Ajustar layout para melhor visualização\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise Exploratória dos Clusters: Gênero\n",
        "\n",
        "A primeira análise exploratória dos clusters focou na variável sexo, com o objetivo de identificar possíveis correlações entre a distribuição de sexo e a segmentação dos clusters. Para isso, foram gerados gráficos de pizza para cada cluster, permitindo uma visualização clara dessa divisão.\n",
        "\n",
        "#### Resultados por Cluster\n",
        "\n",
        "#### Cluster 0\n",
        "- **Distribuição:** 50,6% Homens, 49,4% Mulheres\n",
        "- **Análise:** Observamos uma distribuição equilibrada, indicando que não há uma tendência significativa de preferência por sexo nesse grupo.\n",
        "\n",
        "#### Cluster 1\n",
        "- **Distribuição:** 73,7% Homens, 26,3% Mulheres\n",
        "- **Análise:** A predominância masculina é marcante, sugerindo uma clara inclinação desse grupo para o sexo masculino.\n",
        "\n",
        "#### Cluster 2\n",
        "- **Distribuição:** 60,1% Homens, 39,9% Mulheres\n",
        "- **Análise:** Apresenta também uma maioria masculina, embora em menor proporção em relação ao Cluster 1.\n",
        "\n",
        "#### Cluster 3\n",
        "- **Distribuição:** 66,5% Homens, 33,5% Mulheres\n",
        "- **Análise:** Segue a mesma tendência dos clusters anteriores, com uma maioria masculina.\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "Esses resultados sugerem que, embora alguns clusters tenham uma distribuição de sexo mais equilibrada, há uma tendência geral de predominância masculina, especialmente nos **Clusters 1** e **3**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular custo médio por cluster\n",
        "custos_medios = []\n",
        "clusters = sorted(df_k_means_esc_all['group'].unique())\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "    valor_total = df_cluster['valor_pago_sinistro'].sum()\n",
        "    quantidade_total = df_cluster['quantidade'].sum()\n",
        "    custo_medio = valor_total / quantidade_total\n",
        "    custos_medios.append(custo_medio)\n",
        "cores = ['#3ABD3A']  # Verde escuro\n",
        "# Criar o gráfico\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bars = ax.bar(range(len(clusters)), custos_medios, color= cores)\n",
        "# Adicionar rótulos e título\n",
        "ax.set_xlabel('Clusters')\n",
        "ax.set_ylabel('Custo Médio (R$)')\n",
        "ax.set_title('Custo Médio por Utilização em Cada Cluster')\n",
        "ax.set_xticks(range(len(clusters)))\n",
        "ax.set_xticklabels([f'Cluster {cluster}' for cluster in clusters])\n",
        "# Adicionar os valores em cima das barras\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'R$ {height:.2f}',\n",
        "            ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise de Custo Médio por Utilização em Cada Cluster\n",
        "Nesta análise, buscamos entender a variação no custo médio por utilização entre os diferentes clusters. A visualização gerada por meio de um gráfico de barras permite uma análise clara da disparidade entre os clusters.\n",
        "### Resultados por Cluster\n",
        "#### Cluster 0\n",
        "- **Custo Médio:** R$ 118,09\n",
        "- **Análise:** O Cluster 0 apresenta um custo médio relativamente equilibrado em comparação com os demais clusters.\n",
        "#### Cluster 1\n",
        "- **Custo Médio:** R$ 109,41\n",
        "- **Análise:** O Cluster 1 mantém uma média similar ao Cluster 0, com um custo de utilização um pouco menor, sugerindo pouca variação entre os dois clusters.\n",
        "#### Cluster 2\n",
        "- **Custo Médio:** R$ 95,55\n",
        "- **Análise:** O Cluster 2 tem o custo médio mais baixo entre todos, indicando uma menor utilização ou serviços menos dispendiosos em comparação com os outros clusters.\n",
        "#### Cluster 3\n",
        "- **Custo Médio:** R$ 252,26\n",
        "- **Análise:** O Cluster 3 se destaca significativamente dos demais, apresentando o maior custo médio, mais que o dobro do valor do Cluster 0. Isso sugere que as utilizações associadas a esse grupo são consideravelmente mais caras.\n",
        "### Conclusão\n",
        "A análise sugere que, enquanto os Clusters 0, 1 e 2 possuem custos médios relativamente próximos e equilibrados, o Cluster 3 tem um custo médio de utilização substancialmente mais alto, indicando um comportamento distinto e provavelmente mais caro para os serviços ou produtos associados a esse grupo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos criar quatro gráficos que examinam a distribuição dos planos em cada cluster individualmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters = [0, 1, 2, 3]\n",
        "planCounts = {}\n",
        "\n",
        "# Obter os nomes únicos dos planos\n",
        "unique_plans = df_k_means_esc_all['descricao_plano_sinistro'].unique()\n",
        "\n",
        "# Inicializar listas vazias para cada plano em planCounts\n",
        "for plan in unique_plans:\n",
        "    planCounts[plan] = []\n",
        "\n",
        "# Contar as ocorrências de cada plano por cluster\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "    counts = df_cluster['descricao_plano_sinistro'].value_counts()    \n",
        "    for plan in unique_plans:\n",
        "        planCounts[plan].append(counts.get(plan, 0))\n",
        "\n",
        "# Definir um mapeamento de cores para cada plano para garantir cores consistentes entre os gráficos\n",
        "plan_colors = {plan: f'C{i}' for i, plan in enumerate(unique_plans)}\n",
        "\n",
        "# Criar um gráfico separado para cada cluster, filtrando os planos com 0 registros\n",
        "for cluster in clusters:\n",
        "    # Filtrar os planos com 0 registros para este cluster específico\n",
        "    plans_in_cluster = [plan for plan in unique_plans if planCounts[plan][cluster] > 0]\n",
        "    \n",
        "    # Pular clusters sem planos válidos\n",
        "    if len(plans_in_cluster) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Criar uma lista de tuplas (plano, contagem) para o cluster e ordenar do maior para o menor\n",
        "    plans_in_cluster_sorted = sorted(\n",
        "        [(plan, planCounts[plan][cluster]) for plan in plans_in_cluster], \n",
        "        key=lambda x: x[1], \n",
        "        reverse=False\n",
        "    )\n",
        "    \n",
        "    total_count = sum(plan[1] for plan in plans_in_cluster_sorted)\n",
        "    \n",
        "    # Obter a contagem máxima para este cluster para definir o limite dinâmico\n",
        "    max_count = max(plan[1] for plan in plans_in_cluster_sorted)\n",
        "    \n",
        "    # Definir o limite com um buffer de 1000 unidades\n",
        "    x_limit = max_count + 1000\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    # Plotar as barras horizontais com cores consistentes\n",
        "    for i, (plan, count) in enumerate(plans_in_cluster_sorted):\n",
        "        percentage = (count / total_count) * 100 if total_count > 0 else 0\n",
        "        \n",
        "        # Criar as barras horizontais usando a cor do mapeamento de cores pré-definido\n",
        "        bars = ax.barh(plan, count, color=plan_colors[plan], label=plan, height=0.5)\n",
        "        \n",
        "        # Adicionar a quantidade e a porcentagem em cada barra\n",
        "        for bar in bars:\n",
        "            width = bar.get_width()\n",
        "            \n",
        "            # Exibir a quantidade e a porcentagem em cada barra\n",
        "            ax.annotate(f'{int(width)}',  # Exibir a quantidade\n",
        "                        xy=(width, bar.get_y() + bar.get_height() / 2),  # Posição do texto\n",
        "                        xytext=(2, 5),  # Deslocar o texto levemente\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='left', va='center', fontsize=9, weight='bold')  # Ajustar o tamanho da fonte\n",
        "            \n",
        "            ax.annotate(f'({percentage:.1f}%)',  # Exibir a porcentagem\n",
        "                        xy=(width, bar.get_y() + bar.get_height() / 2),  # Posição do texto\n",
        "                        xytext=(2, -5),  # Deslocar o texto da porcentagem levemente\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='left', va='center', fontsize=8)  # Ajustar o tamanho da fonte\n",
        "\n",
        "    # Definir os títulos e rótulos do gráfico\n",
        "    ax.set_title(f'Distribuição de Planos no Cluster {cluster} (Quantidade e Porcentagem)')\n",
        "    ax.set_xlabel('Quantidade de Planos')\n",
        "    ax.set_ylabel('Planos')\n",
        "    \n",
        "    # Definir um limite específico no eixo x com base no valor máximo e um offset de 1000\n",
        "    ax.set_xlim(0, x_limit)\n",
        "    \n",
        "    # Exibir a legenda\n",
        "    ax.legend(title='Plano', loc='lower right')\n",
        "\n",
        "    # Ajustar o layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise da Distribuição de Planos por Cluster\n",
        "\n",
        "#### Cluster 0\n",
        "No Cluster 0, o plano **TN1E** é o mais predominante, com **42.4%** dos planos, totalizando **2984** unidades. Logo em seguida, o plano **TNQ2** aparece com **25.7%** (1811 planos), e o **TQN2** representa **24.6%** (1733 planos). Já os planos **NP6X**, **NP2X** e **TP8X** possuem uma presença menor, com **3.0%** (209), **2.3%** (163) e **2.0%** (138) respectivamente. Este cluster é fortemente dominado pelos três primeiros planos, que juntos representam mais de 90% dos planos alocados.\n",
        "\n",
        "#### Cluster 1\n",
        "No Cluster 1, o plano **TQN2** é altamente dominante, correspondendo a **83.0%** dos planos, com **12,228** planos no total. O segundo plano mais relevante é o **NP2X**, com **9.4%** (1392 planos), seguido pelo **NP6X** com **4.1%** (597). O plano **TNE1** aparece em uma quantidade reduzida, com **3.4%** (505). Os demais planos, como **TP8X**, **TN1E** e **QN06**, têm uma presença praticamente insignificante, representando menos de **1%** da distribuição de planos neste cluster.\n",
        "\n",
        "#### Cluster 2\n",
        "No Cluster 2, o plano **TQN2** novamente é o mais prevalente, com **67.4%** dos planos, totalizando **9072**. O plano **NP2X** aparece como o segundo mais comum, com **17.3%** (2324), seguido pelo **TNE1** com **7.5%** (1005 planos). O plano **NP6X** ocupa **5.5%** (738 planos) e o **TP8X** apenas **2.3%** (314 planos). Planos como **QN06** e **TNQ2** são basicamente inexistentes nesse cluster, representando **0.1%** e **0.0%** respectivamente.\n",
        "\n",
        "#### Cluster 3\n",
        "No Cluster 3, o plano **TQN2** continua liderando com **67.4%**, somando **2476** planos. Em segundo lugar aparece o **NP2X** com **17.9%** (656), seguido pelo **NP6X** com **9.0%** (330). O plano **TP8X** tem uma pequena presença de **4.0%** (148). Os planos **TN1E**, **TNQ2** e **TNE1** têm presenças não muito significativas, com **0.6%** e **0.4%**, correspondendo a valores muito pequenos (entre 22 e 16 planos).\n",
        "\n",
        "### Conclusão dos planos\n",
        "Em todos os clusters, o plano **TQN2** se destaca como o mais predominante de todos, especialmente no Cluster 1, onde representa a maioria por **83.0%**. Os planos **NP2X** e **NP6X** também aparecem de forma consistente nos clusters, com variações de presença, sendo mais relevantes nos Clusters 1, 2 e 3. Por outro lado, planos como **TP8X**, **TN1E**, **TNQ2** e **QN06** aparecem em quantidades muito pequenas, sem relevância significativa em nenhum dos clusters. Essa distribuição sugere que o **TQN2** é o plano mais utilizado de forma consistente, enquanto os outros planos têm presenças muito mais localizadas e menores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entender melhor as diferenças, iremos criar gráficos de pizza que nos permitirão visualizar a distribuição das empresas em cada cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clusters = [0, 1, 2, 3]\n",
        "companyCounts = {}\n",
        "\n",
        "# Obter os nomes únicos das empresas\n",
        "unique_companies = df_k_means_esc_all['nome_empresa_sinistro'].unique()\n",
        "\n",
        "# Inicializar listas vazias para cada empresa em companyCounts\n",
        "for company in unique_companies:\n",
        "    companyCounts[company] = []\n",
        "\n",
        "# Contar as ocorrências de cada empresa por cluster\n",
        "for cluster in clusters:\n",
        "    df_cluster = df_k_means_esc_all[df_k_means_esc_all['group'] == cluster]\n",
        "    counts = df_cluster['nome_empresa_sinistro'].value_counts()\n",
        "    for company in unique_companies:\n",
        "        companyCounts[company].append(counts.get(company, 0))\n",
        "\n",
        "# Criar uma figura com 4 subplots (2x2 layout)\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Definir as cores para cada empresa (Verde escuro e cinza)\n",
        "colors = ['#3ABD3A', '#D4D6D5']\n",
        "company_colors = {company: colors[i % len(colors)] for i, company in enumerate(unique_companies)}\n",
        "\n",
        "# Inicializar uma lista de legendas (labels) para a tabela\n",
        "all_labels = set()\n",
        "\n",
        "# Criar um gráfico de pizza para cada cluster\n",
        "for i, cluster in enumerate(clusters):\n",
        "    # Filtrar as empresas com 0 registros para este cluster específico\n",
        "    companies_in_cluster = [company for company in unique_companies if companyCounts[company][cluster] > 0]\n",
        "\n",
        "    # Pular clusters sem empresas válidas\n",
        "    if len(companies_in_cluster) == 0:\n",
        "        continue\n",
        "\n",
        "    # Criar uma lista de tuplas (empresa, contagem) para o cluster e ordenar do maior para o menor\n",
        "    companies_in_cluster_sorted = sorted(\n",
        "        [(company, companyCounts[company][cluster]) for company in companies_in_cluster],\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    # Extrair os nomes das empresas e as contagens\n",
        "    labels = [company for company, count in companies_in_cluster_sorted]\n",
        "    sizes = [count for company, count in companies_in_cluster_sorted]\n",
        "\n",
        "    # Atualizar a lista de legendas (para garantir que as cores permaneçam consistentes)\n",
        "    all_labels.update(labels)\n",
        "\n",
        "    # Determinar o subplot a ser usado\n",
        "    ax = axs[i // 2, i % 2]\n",
        "\n",
        "    # Plotar o gráfico de pizza sem bordas\n",
        "    wedges, texts, autotexts = ax.pie(sizes, labels=None, colors=[company_colors[company] for company in labels],\n",
        "                                      autopct='%1.0f%%', startangle=90)\n",
        "\n",
        "    # Aumentar o tamanho da fonte das porcentagens dentro das pizzas\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_fontsize(14)  # Aqui definimos o tamanho da fonte para 14\n",
        "\n",
        "    # Definir o título do gráfico e ajustar a distância com pad\n",
        "    ax.set_title(f'Cluster {cluster}', pad=0)\n",
        "\n",
        "# Ajustar o espaçamento entre os subplots para remover o espaço branco\n",
        "plt.subplots_adjust(wspace=0, hspace=0)  # Reduzir o espaçamento entre as pizzas\n",
        "\n",
        "# Adicionar uma legenda global fora da área dos gráficos\n",
        "fig.legend(wedges, all_labels, title=\"Empresas\", loc=\"center right\", bbox_to_anchor=(1.1, 0.5),\n",
        "           fontsize=12, labelspacing=1.5)  # Aumentar o espaçamento e o tamanho da fonte da legenda\n",
        "\n",
        "# Ajustar o layout para garantir que todos os elementos sejam exibidos corretamente\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise da Distribuição de Empresas por Cluster\n",
        "\n",
        "A **UNIPAR CARBOCLORO S.A.** tem presença dominante nos Clusters 0, 2 e 3, com participações de **98%, 100% e 72%**, respectivamente. Já a **UNIPAR INDUPA DO BRASIL S.A.** possui uma presença pequena de **2% no Cluster 0**, mas é totalmente dominante no **Cluster 1, com 100%**. No **Cluster 3**, a presença é mais equilibrada, com a **UNIPAR INDUPA** detendo **28%**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusão da análise final dos Clusters\n",
        "\n",
        "A base da nossa análise final podemos perceber as principais diferenças entre os Clusters, aqui estão os principais fatores de cada um listado.\n",
        "\n",
        "#### Cluster 0: \n",
        "O Cluster 0 é o terçeiro menor Cluster com em torno de 7 mil segurados, possui um custo de media de R$118.09 sendo a segunda maior media com o custo total de R$874.933,56, é formado apenas por segurados de 54 a 59 anos, 98% sendo da Unipar Indupa e apenas 2% sendo da Carbocloro, possui uma quase perfeita divisão entre sexos, 50.6% sendo Masculino e 49.4% sendo Feminino, o Cluster 0 também é o unico que possui os Planos TN1E e TNQ2 sendo o TN1E o mais prevalente com quase 3 mil planos.\n",
        "\n",
        "#### Cluster 1: \n",
        "O Cluster 1 é o maior Cluster, possuindo perto de 15 mil segurados, possui um custo de média de R$109.41 com o custo total de R$1.777.017,25 sendo o maior custo total entre os Clusters, abrange idades de 18 a 59 anos e é formado por 73.7% por segurados masculino e 26.3% de segurados femininos, possui como seu plano mais utilizado o TQN2, esse Cluster é completamente formado apenas por segurados da Unipar Carbocloro.\n",
        "\n",
        "#### Cluster 2: \n",
        "O Cluster 2 é extremamente semelhante ao Cluster 1, possui em torno de 13 mil segurados tendo sua principal diferença que ao contrario do Cluster 1 que é formado apenas for segurados da Unipar Carbocloro, esse Cluster é completamente formado por apenas segurados da Unipar Indupa, é o segundo maior Cluster, possui um custo de media de R$95.55 com seu custo total de R$1.373.039,73 sendo o segundo maior custo apos o Cluster 1, igual ao Cluster 1 abrange idades de 18 a 59 anos e seu plano mais utilizado é o TQN2, porem possui um aumento na quantidades de segurados de sexo feminino com 39.9% sendo sexo feminino e 60.1% de sexo masculino.\n",
        "\n",
        "#### Cluster 3: \n",
        "O Cluster 3 se diferencia completamente dos outros, sendo apenas formado por tipo de Sinistro de Reembolso enquanto todos os outros são de Rede, possui a menor quantidade de Segurados quase chegando a 4 mil segurados, porem possui o terceiro maior custo de R$1.263.330,72 quase ultrapasando o Cluster 2 com R$1.373.039,73 que possui 13 mil segurados, o custo medio é de R$252.26 sendo o maior custo medio entre todos os clusters, igual ao Clusters 1 e 2 seu plano mais utilizado é o TQN2, 66.5% de seus segurados são do sexo masculino e 33.5% do sexo feminino, é formado por principalmente a Unipar Indupa com 72% de seus segurados sendo de la enqunto 28% são da Carbocloro."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
